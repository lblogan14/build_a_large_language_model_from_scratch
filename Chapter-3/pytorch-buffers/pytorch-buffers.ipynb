{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24df1864",
   "metadata": {},
   "source": [
    "# PyTorch Buffers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c751c",
   "metadata": {},
   "source": [
    "PyTorch buffers are tensor attributes of a `torch.nn.Module` that are not considered parameters, but are still part of the module's state. They are typically used to store tensors that should be saved and loaded with the model, but do not require gradients for optimization. Buffers can be registered using the `register_buffer` method of a `torch.nn.Module`.\n",
    "\n",
    "Buffers in PyTorch are useful when dealing with GPU computations, as they need to be transferred between devices alongside the model's parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c24f2d4",
   "metadata": {},
   "source": [
    "## Example without Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb65783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CausalAttentionWithoutBuffers(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens],\n",
    "            -torch.inf\n",
    "        )\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa806a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "# Prepare dummy inputs\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0) # (b, num_tokens, d_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d70200c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vectors without Buffers:\n",
      "tensor([[[-0.5063,  0.3518],\n",
      "         [-0.6503,  0.3955],\n",
      "         [-0.6976,  0.4064],\n",
      "         [-0.6289,  0.3677],\n",
      "         [-0.6131,  0.3179],\n",
      "         [-0.5870,  0.3259]],\n",
      "\n",
      "        [[-0.5063,  0.3518],\n",
      "         [-0.6503,  0.3955],\n",
      "         [-0.6976,  0.4064],\n",
      "         [-0.6289,  0.3677],\n",
      "         [-0.6131,  0.3179],\n",
      "         [-0.5870,  0.3259]]])\n"
     ]
    }
   ],
   "source": [
    "context_length = batch.shape[1]\n",
    "d_in = batch.shape[2]\n",
    "d_out = 2\n",
    "\n",
    "ca_without_buffer = CausalAttentionWithoutBuffers(d_in, d_out, context_length, dropout=0.0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    context_vecs = ca_without_buffer(batch)\n",
    "\n",
    "print(\"Context Vectors without Buffers:\")\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c15747",
   "metadata": {},
   "source": [
    "This is fine. However, if we transfer between devices, for example, moving from CPU to GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aedd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_cuda = torch.cuda.is_available()\n",
    "has_mps = torch.backends.mps.is_available()\n",
    "\n",
    "print(\"Machine has GPU:\", has_cuda or has_mps)\n",
    "\n",
    "if has_mps:\n",
    "    device = torch.device(\"mps\")   # Apple Silicon GPU (Metal)\n",
    "elif has_cuda:\n",
    "    device = torch.device(\"cuda\")  # NVIDIA GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # CPU fallback\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "batch = batch.to(device)\n",
    "ca_without_buffer = ca_without_buffer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42339687",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    context_vecs = ca_without_buffer(batch)\n",
    "\n",
    "print(\"Context Vectors without Buffers (on device):\")\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100bebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"W_query.device:\", ca_without_buffer.W_query.weight.device)\n",
    "print(\"mask.device:\", ca_without_buffer.mask.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47a7d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "type(ca_without_buffer.mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37f1a45",
   "metadata": {},
   "source": [
    "The `mask` tensor will not be transferred to the GPU, because it is not a PyTorch parameter like the weights, so we have to manually move it to the GPU via `mask.to(device)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0f90e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_without_buffer.mask = ca_without_buffer.mask.to(device)\n",
    "print(\"mask.device:\", ca_without_buffer.mask.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd1168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    context_vecs = ca_without_buffer(batch)\n",
    "\n",
    "print(\"Context Vectors without Buffers (on device):\")\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ea83e1",
   "metadata": {},
   "source": [
    "## Example with Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c1deba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CausalAttentionWithBuffers(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Old way: self.mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        # New way: register the mask as a buffer\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens],\n",
    "            -torch.inf\n",
    "        )\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa862b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_query.device: cpu\n",
      "mask.device: cpu\n"
     ]
    }
   ],
   "source": [
    "ca_with_buffer = CausalAttentionWithBuffers(d_in, d_out, context_length, 0.0)\n",
    "ca_with_buffer.to(device)\n",
    "\n",
    "print(\"W_query.device:\", ca_with_buffer.W_query.weight.device)\n",
    "print(\"mask.device:\", ca_with_buffer.mask.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7de81071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3550, -0.6560],\n",
      "         [-0.1536, -0.7514],\n",
      "         [-0.0853, -0.7803],\n",
      "         [-0.0297, -0.7015],\n",
      "         [-0.0417, -0.6247],\n",
      "         [-0.0040, -0.6322]],\n",
      "\n",
      "        [[-0.3550, -0.6560],\n",
      "         [-0.1536, -0.7514],\n",
      "         [-0.0853, -0.7803],\n",
      "         [-0.0297, -0.7015],\n",
      "         [-0.0417, -0.6247],\n",
      "         [-0.0040, -0.6322]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    context_vecs = ca_with_buffer(batch)\n",
    "\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3627d924",
   "metadata": {},
   "source": [
    "## Buffers and `state_dict`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafaf93f",
   "metadata": {},
   "source": [
    "The PyTorch buffers get included in a model's `state_dict`, which is a dictionary containing all the parameters and buffers of the model. This means that when you save a model using `torch.save(model.state_dict(), 'model.pth')`, both the parameters and buffers will be saved. When you load the model using `model.load_state_dict(torch.load('model.pth'))`, both the parameters and buffers will be restored to their saved state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d5d3cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('W_query.weight',\n",
       "              tensor([[-0.0043,  0.3097, -0.4752],\n",
       "                      [-0.4249, -0.2224,  0.1548]])),\n",
       "             ('W_key.weight',\n",
       "              tensor([[-0.0114,  0.4578, -0.0512],\n",
       "                      [ 0.1528, -0.1745, -0.1135]])),\n",
       "             ('W_value.weight',\n",
       "              tensor([[-0.5516, -0.3824, -0.2380],\n",
       "                      [ 0.0214,  0.2282,  0.3464]]))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_without_buffer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "141b01d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('mask',\n",
       "              tensor([[0., 1., 1., 1., 1., 1.],\n",
       "                      [0., 0., 1., 1., 1., 1.],\n",
       "                      [0., 0., 0., 1., 1., 1.],\n",
       "                      [0., 0., 0., 0., 1., 1.],\n",
       "                      [0., 0., 0., 0., 0., 1.],\n",
       "                      [0., 0., 0., 0., 0., 0.]])),\n",
       "             ('W_query.weight',\n",
       "              tensor([[-0.3914, -0.2514,  0.2097],\n",
       "                      [ 0.4794, -0.1188,  0.4320]])),\n",
       "             ('W_key.weight',\n",
       "              tensor([[-0.0931,  0.0611,  0.5228],\n",
       "                      [-0.5356, -0.3635, -0.1462]])),\n",
       "             ('W_value.weight',\n",
       "              tensor([[-0.2251,  0.4988, -0.3742],\n",
       "                      [-0.2658, -0.4034, -0.5407]]))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_with_buffer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827926ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
