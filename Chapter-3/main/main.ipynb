{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80cb7369",
   "metadata": {},
   "source": [
    "# Chapter 3: Coding Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "469afd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "print(\"torch version:\", version(\"torch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281306c7",
   "metadata": {},
   "source": [
    "## 3.3 Attending to Different Parts of the Input with Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83989c2",
   "metadata": {},
   "source": [
    "### 3.3.1 A Simple Self-Attention Mechanism without Trainable Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff26a97",
   "metadata": {},
   "source": [
    "Assume we have an input sequence, denoted as *x*, consisting of *T* elements represented as *x(1)* to *x(T)*. For example, in natural language processing, these elements could represent words or tokens in a sentence but have already been transformed into embeddings. \n",
    "\n",
    "Consider an input text like \"Your journey starts with one step.\" Each element of the sequence, such as *x(1)*, corresponds to a *d*-dimensional embedding vector representing a specific token, such as \"Your\".\n",
    "\n",
    "In self-attention, our goal is to calculate context vectors *z(i)* for each element *x(i)* in the input sequence(, where *z* and *x* have the same dimension). A **context vector** can be interepreted as an enriched embedding that captures not only the information from the token itself but also relevant information from other tokens in the sequence.\n",
    "\n",
    "The concept of context vectors is essential in LLMs, which need to understand the relationships and relevance of words in a sentence to each other. A context vector *z(i)* is a weighted sum over the inputs *x(1)* to *x(T)*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3303cc4c",
   "metadata": {},
   "source": [
    "For example, suppose we focus on the embedding vector *x(2)*, which corresponds to the token \"journey\". This context vector *z(2)* is a weighted sum over all inputs *x(1)* to *x(T)* weighted with respect to the second input element *x(2)*. \n",
    "\n",
    "The attention weights are the weights that determine how much each of the input elements contributes to the weighted sum when computing *z(2)*.\n",
    "\n",
    "By convention, the unnormalized attention weights are referred to as **attention scores**, whereas the normalized weights are called **attention weights**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4941bc",
   "metadata": {},
   "source": [
    "Suppose we have the following input sentence that is already embedded in 3-dimensional vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9662b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "     [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "     [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "     [0.22, 0.58, 0.33], # with     (x^4)\n",
    "     [0.77, 0.25, 0.10], # one      (x^5)\n",
    "     [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322bf1e1",
   "metadata": {},
   "source": [
    "**Step 1**: compute unnormalized attention scores *w*.\n",
    "\n",
    "Suppose we use *x(2)* as the query *q(2)*, then we can compute the unnormalized attention scores via dot products:\n",
    "- $w(2,1) = x(1)q(2)^T$\n",
    "- $w(2,2) = x(2)q(2)^T$\n",
    "- $w(2,3) = x(3)q(2)^T$\n",
    "- ...\n",
    "- $w(2,T) = x(T)q(2)^T$\n",
    "\n",
    "where $w(2,1)$ tells us the input sequence element 2 was used as a query against input sequence element 1.\n",
    "\n",
    "Now we can compute the unnormalized attention scores by computing the dot product between the query *x(2)* and all other input tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccea7d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of attn_scores_2: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 2nd input token \"journey\"\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "print(\"Shape of attn_scores_2:\", attn_scores_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf8834a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores for the 2nd input token 'journey':\n",
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "for i, x_i in enumerate(inputs):\n",
    "    # dot product\n",
    "    # (transpose not necessary here since x_i and query are 1D tensors)\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(\"Attention scores for the 2nd input token 'journey':\")\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459e24a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product computed manually: tensor(0.9544)\n",
      "Dot product computed with torch.dot: tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "# This is totally equivalent:\n",
    "res = 0.\n",
    "\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx] * query[idx]\n",
    "\n",
    "print(\"Dot product computed manually:\", res)\n",
    "print(\"Dot product computed with torch.dot:\", torch.dot(inputs[0], query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f126392",
   "metadata": {},
   "source": [
    "**Step 2**: normalize the unnormalized attention scores so that they sum up to 1.\n",
    "\n",
    "This normalization is a convention that is useful for interpretation and maintaining training stability in an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "345ec4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights for the 2nd input token 'journey':\n",
      "tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights for the 2nd input token 'journey':\")\n",
    "print(attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d5f53e",
   "metadata": {},
   "source": [
    "However in practice, we use the **softmax** function for normalization, which is better at handling extreme values and has more desirable gradient properties during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dcf6e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights for the 2nd input token 'journey' using naive softmax:\n",
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights for the 2nd input token 'journey' using naive softmax:\")\n",
    "print(attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31f4dc1",
   "metadata": {},
   "source": [
    "Softmax ensures the attention weights are always positive and sum to 1, making the output interpretable as probabilities or relative importance.\n",
    "\n",
    "To avoid numerical instability (overflow/underflow) when computing the exponential of large or small values, we prefer the PyTorch built-in softmax function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2920b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights for the 2nd input token 'journey' using PyTorch softmax:\n",
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights for the 2nd input token 'journey' using PyTorch softmax:\")\n",
    "print(attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c050ba",
   "metadata": {},
   "source": [
    "**Step 3**: compute the context vector *z(2)* by multiplying the embedded input tokens *x(1)* to *x(T)* with the attention weights *a(2,1)* to *a(2,T)* and sum the resulting vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "430f0ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector for the 2nd input token 'journey':\n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # 2nd input token \"journey\"\n",
    "\n",
    "context_vector_2 = torch.zeros(query.shape) # initialize context vector\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vector_2 += attn_weights_2[i] * x_i\n",
    "\n",
    "print(\"Context vector for the 2nd input token 'journey':\")\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7d6aa3",
   "metadata": {},
   "source": [
    "### 3.3.2 Computing Attention Weights for All Input Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad0e2a",
   "metadata": {},
   "source": [
    "We have computed the attention weights and context vector for the 2nd input token \"journey\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adc0a795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights for the 2nd input token 'journey':\n",
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n"
     ]
    }
   ],
   "source": [
    "print(\"Attention weights for the 2nd input token 'journey':\")\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82904f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector for the 2nd input token 'journey':\n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"Context vector for the 2nd input token 'journey':\")\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0af29cf",
   "metadata": {},
   "source": [
    "Now we generalize this process to compute attention weights and context vectors for all input tokens in the sequence. This involves repeating the steps outlined above for each token in the input sequence, treating each token as a query in turn.\n",
    "\n",
    "In self-attention, this process starts with the calculation of attention scores, which are subsequently normalized to derive attention weights that sum to one. Later, these attention weights are used to generate the context vectors through a weighted sum of the input embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082e038f",
   "metadata": {},
   "source": [
    "**Step 1**: compute the unnormalized attention scores *w* for each input token as a query against all input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1924bc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores matrix:\n",
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty((inputs.shape[0], inputs.shape[0]))\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(\"Attention scores matrix:\")\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db0b81",
   "metadata": {},
   "source": [
    "Each element in the tensor represents the attention score between each pair of input tokens. We can achieve the same result using matrix multiplication for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ded95d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores matrix using matrix multiplication:\n",
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "\n",
    "print(\"Attention scores matrix using matrix multiplication:\")\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dbe274",
   "metadata": {},
   "source": [
    "**Step 2**: normalize each row so that the values in each row sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e05fc13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights matrix:\n",
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "print(\"Attention weights matrix:\")\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7789775a",
   "metadata": {},
   "source": [
    "We can verify that the rows all sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d99b340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of attention weights for the 2nd input token 'journey': tensor(1.)\n",
      "Sum of attention weights for all input tokens: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum(attn_weights[1])\n",
    "print(\"Sum of attention weights for the 2nd input token 'journey':\", row_2_sum)\n",
    "\n",
    "all_rows_sum = attn_weights.sum(dim=-1)\n",
    "print(\"Sum of attention weights for all input tokens:\", all_rows_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3c0565",
   "metadata": {},
   "source": [
    "**Step 3**: compute all context vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5e1dd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All context vectors:\n",
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "\n",
    "print(\"All context vectors:\")\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caea82ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous context vector for the 2nd input token 'journey':\n",
      "tensor([0.4419, 0.6515, 0.5683])\n",
      "Context vector from all_context_vecs for the 2nd input token 'journey':\n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"Previous context vector for the 2nd input token 'journey':\")\n",
    "print(context_vector_2)\n",
    "\n",
    "print(\"Context vector from all_context_vecs for the 2nd input token 'journey':\")\n",
    "print(all_context_vecs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f34f1",
   "metadata": {},
   "source": [
    "## 3.4 Implementing Self-Attention with Trainable Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df5ecba",
   "metadata": {},
   "source": [
    "### 3.4.1 Computing the Attention Weights Step-by-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807ea149",
   "metadata": {},
   "source": [
    "We will implement the self-attention mechanism by introducing three trainable weight matrices: $W_{query}$, $W_{key}$, and $W_{value}$, which are used to project the input embeddings, $x^{(i)}$, into three different spaces: queries, keys, and values, respectively. These weight matrices are learned and can be used to capture different aspects of the input data:\n",
    "- **Query vector**: $q^{(i)} = x^{(i)} W_{query}$\n",
    "- **Key vector**: $k^{(i)} = x^{(i)} W_{key}$\n",
    "- **Value vector**: $v^{(i)} = x^{(i)} W_{value}$\n",
    "\n",
    "For example, if we still use the input text \"Your journey starts with one step.\" and focus on the second token \"journey\" and assume the embedding vector $x^{(2)}$ corresponds to \"journey\", we can compute the query, key, and value vectors as follows:\n",
    "- $q^{(2)} = x^{(2)} W_{query}$\n",
    "- $k^{(2)} = x^{(2)} W_{key}$\n",
    "- $v^{(2)} = x^{(2)} W_{value}$\n",
    "\n",
    "But for other tokens, we can only compute their key and value vectors (since they are not used as queries in this step):\n",
    "- For the first token \"Your\":\n",
    "    - $k^{(1)} = x^{(1)} W_{key}$\n",
    "    - $v^{(1)} = x^{(1)} W_{value}$\n",
    "- For the last token \"step.\":\n",
    "    - $k^{(T)} = x^{(T)} W_{key}$\n",
    "    - $v^{(T)} = x^{(T)} W_{value}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec903d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second input token \"journey\"\n",
    "x_2 = inputs[1]\n",
    "\n",
    "# the input embedding size, d=3\n",
    "d_in = inputs.shape[1]\n",
    "# the output embedding size, d=2\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c013b2",
   "metadata": {},
   "source": [
    "For demo purposes, we assume the output embedding size is 2, so the weight matrices have the following shapes:\n",
    "- $W_{query}$: (3, 2)\n",
    "- $W_{key}$: (3, 2)\n",
    "- $W_{value}$: (3, 2)\n",
    "\n",
    "But in GPT-like models, the output embedding size is usually the same as the input embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d4c976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query vector for 'journey': tensor([0.5528, 0.9559])\n",
      "Key vector for 'journey': tensor([0.8962, 1.3083])\n",
      "Value vector for 'journey': tensor([0.7284, 1.0720])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "# Initialize weight matrices with random values\n",
    "# (Set `requires_grad=False` for demo purposes)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "# Compute query, key, and value vectors for x_2\n",
    "query_2 = x_2 @ W_query  # shape: (d_out,)\n",
    "key_2 = x_2 @ W_key      # shape: (d_out,)\n",
    "value_2 = x_2 @ W_value  # shape: (d_out,)\n",
    "\n",
    "print(\"Query vector for 'journey':\", query_2)\n",
    "print(\"Key vector for 'journey':\", key_2)\n",
    "print(\"Value vector for 'journey':\", value_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b658505",
   "metadata": {},
   "source": [
    "NOTE: These weight matrices are not the attention weights; they are trainable parameters used to compute the query, key, and value vectors from the input embeddings. The attention weights are computed later using the dot products of the query and key vectors.\n",
    "\n",
    "Next, we will compute the key and value vectors for all input tokens as they are needed to compute the attention weights with respect to the query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde35de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys shape: torch.Size([6, 2])\n",
      "values shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key    # shape: (num_tokens, d_out)\n",
    "values = inputs @ W_value  # shape: (num_tokens, d_out)\n",
    "\n",
    "print(\"keys shape:\", keys.shape)\n",
    "print(\"values shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6824e778",
   "metadata": {},
   "source": [
    "We successfully projected the 6-dimensional input embeddings into 2-dimensional key and value vectors for all input tokens. The next step is to compute the attention scores by computing the dot products between the query and each key vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9af12e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention score for 'journey' with itself: tensor(1.7460)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]  # key vector for \"journey\"\n",
    "attn_scores_22 = query_2.dot(keys_2)\n",
    "\n",
    "print(\"Attention score for 'journey' with itself:\", attn_scores_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51656d8",
   "metadata": {},
   "source": [
    "Then we can generalize this process to all attention scores for the query vector corresponding to the token \"journey\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98188515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores for 'journey' with all tokens:\n",
      " tensor([1.1268, 1.7460, 1.7399, 0.9351, 1.1402, 1.0587])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # shape: (num_tokens,)\n",
    "print(\"Attention scores for 'journey' with all tokens:\\n\", attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576ccd19",
   "metadata": {},
   "source": [
    "Then we need to normalize the attention scores by scaling them and using the softmax function. Here, we will scale the attention scores by dividing them by the square root of the embedding dimension of the key vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71c01727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights for 'journey' with all tokens:\n",
      " tensor([0.1443, 0.2236, 0.2227, 0.1261, 0.1457, 0.1376])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[1] # embedding dimension of the key vectors\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(\"Attention weights for 'journey' with all tokens:\\n\", attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08659970",
   "metadata": {},
   "source": [
    "The reason for the normalization by the embedding dimension size is to **improve the training performance by avoiding small gradients.** When scaling up the embedding dimension, which is typically greater than 1,000 for GPT-like LLMs, large dot products can result in very small gradients during backpropagation due to the softmax function applied to them. As dot products increase, the softmax function behaves more like a *step function*, resulting in gradients **nearing zeros**.\n",
    "\n",
    "The final step is to compute the context vectors as a weighted sum over the value vectors. The attention weights serve as a weigthting factor that weighs the respective importance of each value vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e4fb03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector for 'journey':\n",
      " tensor([0.5780, 0.8419])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(\"Context vector for 'journey':\\n\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa1297a",
   "metadata": {},
   "source": [
    "The concepts of \"key\", \"query\", and \"value\" in the Self-Attention Mechanism are inspired by information retrieval systems, where similar terminology is used to \"store\", \"search\", and \"retrieve\" information.\n",
    "- *Key* is like a database key used for indexing and searching. Each item in the input sequence (e.g., each word in a sentence) has an associated key. These keys are used to match the query.\n",
    "- *Query* is like a search query in a database. It represents the current item (e.g., a word or token in a sentence) the model focuses on or tries to understand. It is used to determine how much attention to pay to other items in the sequence.\n",
    "- *Value* is like the value in a key-value pair in a database. It represents the actual content or representation of the input items. Once the model determines which keys (and thus which parts of the input) are the most relevant to the query (the current focus item), it retrieves the corresponding values to construct a meaningful representation (context vector) for that item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f188a672",
   "metadata": {},
   "source": [
    "### 3.4.2 Implementing a Compact Self-Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afb314cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ebde0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key      # shape: (num_tokens, d_out)\n",
    "        queries = x @ self.W_query  # shape: (num_tokens, d_out)\n",
    "        values = x @ self.W_value   # shape: (num_tokens, d_out)\n",
    "\n",
    "        attn_scores = queries @ keys.T # shape: (num_tokens, num_tokens)\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5,\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values  # shape: (num_tokens, d_out)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b47654ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vectors from SelfAttention_v1:\n",
      " tensor([[0.5762, 0.8392],\n",
      "        [0.5780, 0.8419],\n",
      "        [0.5780, 0.8420],\n",
      "        [0.5628, 0.8183],\n",
      "        [0.5704, 0.8302],\n",
      "        [0.5635, 0.8196]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "sa_v1 = SelfAttention_v1(d_in=d_in, d_out=d_out)\n",
    "print(\"Context vectors from SelfAttention_v1:\\n\", sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e598adc1",
   "metadata": {},
   "source": [
    "Self-attention involves the trainable weight matrices $W_{query}$, $W_{key}$, and $W_{value}$, which are used to project the input embeddings into query, key, and value vectors, respectively.\n",
    "\n",
    "We can further improve the `SelfAttention_v1` class by using PyTorch's `nn.Linear` layers, which effectively perform matrix multiplications when the bias units are disabled. A significant advantage of using `nn.Linear` layers instead of manually implementing `nn.Parameter` is that `nn.Linear` has an optimized weight initialization scheme, contributing to more stable and efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43e1c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out,  bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out,  bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)     # shape: (num_tokens, d_out)\n",
    "        queries = self.W_query(x)  # shape: (num_tokens, d_out)\n",
    "        values = self.W_value(x)   # shape: (num_tokens, d_out)\n",
    "\n",
    "        attn_scores = queries @ keys.T # shape: (num_tokens, num_tokens)\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5,\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values  # shape: (num_tokens, d_out)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abdee29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vectors from SelfAttention_v2:\n",
      " tensor([[-0.5844,  0.3235],\n",
      "        [-0.5871,  0.3269],\n",
      "        [-0.5871,  0.3269],\n",
      "        [-0.5873,  0.3265],\n",
      "        [-0.5876,  0.3276],\n",
      "        [-0.5870,  0.3259]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "sa_v2 = SelfAttention_v2(d_in=d_in, d_out=d_out)\n",
    "print(\"Context vectors from SelfAttention_v2:\\n\", sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb37bbe",
   "metadata": {},
   "source": [
    "## 3.5 Hiding Future Words with Causal Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0531633",
   "metadata": {},
   "source": [
    "### 3.5.1 Applying a Causal Attention Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c87d89",
   "metadata": {},
   "source": [
    "Causal self-attention ensures that the model's prediction for a certain position in a sequence is only dependent on the known outputs at previous positions, not on future positions. To achieve this, for each given token, we should mask out the future tokens (the ones that come after the current token in the input text).\n",
    "\n",
    "For example, in the first step, we use the attention scores and attention weights from the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8798730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights matrix from SelfAttention_v2:\n",
      " tensor([[0.1762, 0.1616, 0.1619, 0.1662, 0.1712, 0.1630],\n",
      "        [0.1665, 0.1678, 0.1675, 0.1669, 0.1614, 0.1699],\n",
      "        [0.1664, 0.1679, 0.1676, 0.1670, 0.1612, 0.1700],\n",
      "        [0.1654, 0.1679, 0.1677, 0.1669, 0.1631, 0.1689],\n",
      "        [0.1645, 0.1691, 0.1686, 0.1671, 0.1595, 0.1713],\n",
      "        [0.1666, 0.1671, 0.1670, 0.1668, 0.1648, 0.1678]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Reuse the query and key weight matrices from sa_v2\n",
    "# to compute the attention scores matrix and attention weights matrix\n",
    "queries = sa_v2.W_query(inputs)  # shape: (num_tokens, d_out)\n",
    "keys = sa_v2.W_key(inputs)       # shape: (num_tokens, d_out\n",
    "\n",
    "# Attention scores matrix\n",
    "attn_scores = queries @ keys.T\n",
    "# Attention weights matrix\n",
    "attn_weights = torch.softmax(\n",
    "    attn_scores / keys.shape[-1]**0.5,\n",
    "    dim=-1\n",
    ")\n",
    "\n",
    "print(\"Attention weights matrix from SelfAttention_v2:\\n\", attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003b3772",
   "metadata": {},
   "source": [
    "The simplest way to mask out future attention weights is by creating a mask with PyTorch's `tril` function with elements below the main diagonal (including the diagonal itself) set to 1 and above the main diagonal set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0e987da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple causal attention mask:\n",
      " tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]  # number of tokens in the input sequence\n",
    "\n",
    "# `tril` creates a lower triangular matrix\n",
    "mask_simple = torch.tril(\n",
    "    torch.ones((context_length, context_length))\n",
    ")\n",
    "print(\"Simple causal attention mask:\\n\", mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e67a22e",
   "metadata": {},
   "source": [
    "Multiply this mask with the attention weights to zero-out the values above the main diagonal, effectively preventing the model from attending to future tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d87f4dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked attention weights matrix:\n",
      " tensor([[0.1762, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1665, 0.1678, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1664, 0.1679, 0.1676, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1654, 0.1679, 0.1677, 0.1669, 0.0000, 0.0000],\n",
      "        [0.1645, 0.1691, 0.1686, 0.1671, 0.1595, 0.0000],\n",
      "        [0.1666, 0.1671, 0.1670, 0.1668, 0.1648, 0.1678]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "print(\"Masked attention weights matrix:\\n\", masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a1bf8",
   "metadata": {},
   "source": [
    "The next step is to **re-normalize** the attention weights to sum up to 1 again in each row after applying the causal mask. This step is crucial because the masking process alters the original attention weights, and re-normalization ensures that the remaining weights still represent a valid probability distribution over the allowed tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ef9e18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-normalized masked attention weights matrix:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4981, 0.5019, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3316, 0.3345, 0.3339, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2476, 0.2514, 0.2511, 0.2498, 0.0000, 0.0000],\n",
      "        [0.1985, 0.2040, 0.2035, 0.2016, 0.1925, 0.0000],\n",
      "        [0.1666, 0.1671, 0.1670, 0.1668, 0.1648, 0.1678]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_normalized = masked_simple / row_sums\n",
    "print(\"Re-normalized masked attention weights matrix:\\n\", masked_simple_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4e5fe1",
   "metadata": {},
   "source": [
    "We can still improve the implementation of the causal attention.\n",
    "\n",
    "The softmax function converts its inputs into a probability distribution. When negative infinity values are present in a row, the softmax function treats them as zero probabilities: $e^{-\\infty} = 0$. Thus, we can implement a more efficient masking \"trick\" by masking the unnormalized attention scores above the main diagonal with negative infinity before they enter the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9638c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(\n",
    "    torch.ones(context_length, context_length),\n",
    "    diagonal=1\n",
    ")\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c07c8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked attention scores matrix with -inf:\n",
      " tensor([[-0.0021,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.0191,  0.0297,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.0196,  0.0319,  0.0293,    -inf,    -inf,    -inf],\n",
      "        [ 0.0109,  0.0323,  0.0306,  0.0234,    -inf,    -inf],\n",
      "        [ 0.0231,  0.0619,  0.0585,  0.0451, -0.0203,    -inf],\n",
      "        [ 0.0068,  0.0113,  0.0104,  0.0086, -0.0085,  0.0175]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(\"Masked attention scores matrix with -inf:\\n\", masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e37a7d6",
   "metadata": {},
   "source": [
    "Now all we need to do is apply the softmax function to these masked results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6939a422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights matrix after applying -inf mask:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4981, 0.5019, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3316, 0.3345, 0.3339, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2476, 0.2514, 0.2511, 0.2498, 0.0000, 0.0000],\n",
      "        [0.1985, 0.2040, 0.2035, 0.2016, 0.1925, 0.0000],\n",
      "        [0.1666, 0.1671, 0.1670, 0.1668, 0.1648, 0.1678]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(\n",
    "    masked / keys.shape[-1]**0.5,\n",
    "    dim=-1\n",
    ")\n",
    "print(\"Attention weights matrix after applying -inf mask:\\n\", attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f0c8b",
   "metadata": {},
   "source": [
    "As we can see based on the output, the values in each row sum to 1, and no further re-normalization is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f23cb1",
   "metadata": {},
   "source": [
    "### 3.5.2 Masking Additional Attention Weights with Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91659896",
   "metadata": {},
   "source": [
    "Dropout is used to prevent overfitting during training by randomly setting a fraction of the input units to zero at each update during training time, which helps to break up co-adaptations among neurons.\n",
    "\n",
    "In the transformer architecture, dropout in the attention mechanism is applied at two specific times:\n",
    "- after calculating the attention weights or \n",
    "- after applying the attention weights to the value vectors.\n",
    "\n",
    "For demo purposes, we will apply the dropout after computing the attention weights with a dropout rate of 50%.\n",
    "\n",
    "When applying dropout to an attention weight matrix with a dropout rate of 50%, half of the elements in the matrix are randomly set to zero. To compensate for the reduction in active elements, the values of the remaining elements in the matrix are scaled up by a factor of `1 / (1 - dropout_rate)` which in this case is `1 / (1 - 0.5) = 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70c05f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights matrix after applying dropout:\n",
      " tensor([[0., 0., 2., 0., 0., 0.],\n",
      "        [2., 2., 0., 2., 2., 2.],\n",
      "        [2., 0., 2., 2., 2., 2.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 0., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.5) # Apply dropout with a rate of 50%\n",
    "example = torch.ones(6, 6) # Example attention weights matrix\n",
    "dropped_out = dropout(example)\n",
    "\n",
    "print(\"Attention weights matrix after applying dropout:\\n\", dropped_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0722cd9a",
   "metadata": {},
   "source": [
    "Apply dropout to the attention weights matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "296cb557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply dropout to the attention weights matrix:\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9963, 1.0037, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6632, 0.0000, 0.6678, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5022, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3969, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3340, 0.3335, 0.3295, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "print(\"Apply dropout to the attention weights matrix:\")\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca1e8c0",
   "metadata": {},
   "source": [
    "### 3.5.3 Implementing a Compact Causal Attention Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ff940",
   "metadata": {},
   "source": [
    "We will now implement a compact `CausalAttention` class that incorporates the causal masking and dropout mechanisms into the self-attention process. We need to make sure this class can handle batches consisting of more than one input sequence so that it supports the batch outputs produced by the data loader during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba6c820b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "# For demo purposes we duplicate the input text sequence\n",
    "batch = torch.stack((inputs, inputs), dim=0)  # shape: (batch_size, num_tokens, d_in)\n",
    "print(\"Input batch shape:\", batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0119d9c7",
   "metadata": {},
   "source": [
    "This means that we have a 3-dimensional tensor consisting of *two* input sequences with *six* tokens each, where each token is represented by a *three*-dimensional embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fb7928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # Causal mask\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(\n",
    "                torch.ones(context_length, context_length),\n",
    "                diagonal=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # batch size, number of tokens, input embedding size\n",
    "\n",
    "        # For inputs where `num_tokens` < `context_length`, this will result in errors\n",
    "        # in the mask creation further below.\n",
    "        # In practice, this is not a problem since the LLM ensures that\n",
    "        # inputs do not exceed `context_length` before reaching this forward method.\n",
    "        keys = self.W_key(x)       # shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)   # shape: (b, num_tokens, d_out)\n",
    "        values = self.W_value(x)    # shape: (b, num_tokens, d_out)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # shape: (b, num_tokens, num_tokens)\n",
    "        # New operations for causal masking\n",
    "        # `_` ops are in-place operations\n",
    "        attn_scores.masked_fill_(\n",
    "            # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "            self.mask.bool()[:num_tokens, :num_tokens],\n",
    "            -torch.inf\n",
    "        )\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5,\n",
    "            dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # Apply dropout to attention weights\n",
    "\n",
    "        context_vec = attn_weights @ values # shape: (b, num_tokens, d_out)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2ec163",
   "metadata": {},
   "source": [
    "The use of `register_buffer` in PyTorch is to store tensors that are not parameters of the model but should still be part of the model's state. This is particularly useful for tensors that are used in computations but do not require gradients, such as masks or fixed constants. By registering a tensor as a buffer, it will be included in the model's state dictionary, allowing it to be saved and loaded along with the model's parameters.\n",
    "\n",
    "For example, when we use the `CausalAttention` class in an LLM, buffers are automatically moved to the appropriate device (CPU or GPU) along with our model, which will be relevant when training our LLM, so that we don't need to manually ensure these tensors are on the same device as our model parameters, avoiding device mismatch errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82484996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vectors from CausalAttention:\n",
      " tensor([[[-0.5063,  0.3518],\n",
      "         [-0.6503,  0.3955],\n",
      "         [-0.6976,  0.4064],\n",
      "         [-0.6289,  0.3677],\n",
      "         [-0.6131,  0.3179],\n",
      "         [-0.5870,  0.3259]],\n",
      "\n",
      "        [[-0.5063,  0.3518],\n",
      "         [-0.6503,  0.3955],\n",
      "         [-0.6976,  0.4064],\n",
      "         [-0.6289,  0.3677],\n",
      "         [-0.6131,  0.3179],\n",
      "         [-0.5870,  0.3259]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Context vectors shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "context_length = batch.shape[1] # number of tokens in the input sequence\n",
    "ca = CausalAttention(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    context_length=context_length,\n",
    "    dropout=0.0\n",
    ")\n",
    "context_vecs = ca(batch)\n",
    "print(\"Context vectors from CausalAttention:\\n\", context_vecs)\n",
    "print(\"Context vectors shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231bc518",
   "metadata": {},
   "source": [
    "## 3.6 Extending Single-Head Attention to Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b6c8bd",
   "metadata": {},
   "source": [
    "### 3.6.1 Stacking Multiple Single-Head Attention Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58ce1d6",
   "metadata": {},
   "source": [
    "The main idea behind multi-head attention is to run the attention mechanism multiple times (in parallel) with different, learned linear projections of the queries, keys, and values. Each of these parallel attention mechanisms is referred to as a \"head.\"\n",
    "\n",
    "We can achieve this by implementing a simple `MultiHeadAttentionWrapper` class that stacks multiple `CausalAttention` layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c7796c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                CausalAttention(\n",
    "                    d_in=d_in,\n",
    "                    d_out=d_out,\n",
    "                    context_length=context_length,\n",
    "                    dropout=dropout,\n",
    "                    qkv_bias=qkv_bias\n",
    "                )\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Concatenate the outputs from all heads\n",
    "        head_outputs = torch.cat(\n",
    "            [head(x) for head in self.heads],\n",
    "            dim=-1\n",
    "        )\n",
    "        return head_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a08c69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vectors from MultiHeadAttentionWrapper:\n",
      " tensor([[[-0.5063,  0.3518, -0.3550, -0.6560],\n",
      "         [-0.6503,  0.3955, -0.1536, -0.7514],\n",
      "         [-0.6976,  0.4064, -0.0853, -0.7803],\n",
      "         [-0.6289,  0.3677, -0.0297, -0.7015],\n",
      "         [-0.6131,  0.3179, -0.0417, -0.6247],\n",
      "         [-0.5870,  0.3259, -0.0040, -0.6322]],\n",
      "\n",
      "        [[-0.5063,  0.3518, -0.3550, -0.6560],\n",
      "         [-0.6503,  0.3955, -0.1536, -0.7514],\n",
      "         [-0.6976,  0.4064, -0.0853, -0.7803],\n",
      "         [-0.6289,  0.3677, -0.0297, -0.7015],\n",
      "         [-0.6131,  0.3179, -0.0417, -0.6247],\n",
      "         [-0.5870,  0.3259, -0.0040, -0.6322]]], grad_fn=<CatBackward0>)\n",
      "Context vectors shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "context_length = batch.shape[1] # number of tokens in the input sequence\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    context_length=context_length,\n",
    "    dropout=0.0,\n",
    "    num_heads=2\n",
    ")\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "print(\"Context vectors from MultiHeadAttentionWrapper:\\n\", context_vecs)\n",
    "print(\"Context vectors shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0143e478",
   "metadata": {},
   "source": [
    "Here we set `num_heads=2` and `d_out=2`, meaning that each head will produce an output embedding of size 2. Since we have 2 heads, the combined output embedding size will be `2 heads * 2 dimensions/head = 4 dimensions`. This is why the final output shape is `(2, 6, 4)`, reflecting the batch size of 2, sequence length of 6, and combined output embedding size of 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f4217f",
   "metadata": {},
   "source": [
    "### 3.6.2 Implementing Multi-Head Attention with Weight Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f7a1cb",
   "metadata": {},
   "source": [
    "We will implement a single `MultiHeadAttention` class to make the multi-head attention mechanism more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4fda21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        # Reduce the projection dim to match desired output dim\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # Linear layer to combine head outputs\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(\n",
    "                torch.ones(context_length, context_length),\n",
    "                diagonal=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # batch size, number of tokens, input embedding size\n",
    "        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`, \n",
    "        # this will result in errors in the mask creation further below. \n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n",
    "        # do not exceed `context_length` before reaching this forward method.\n",
    "\n",
    "        keys = self.W_key(x)      # shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)  # shape: (b, num_tokens, d_out)\n",
    "        values = self.W_value(x)   # shape: (b, num_tokens, d_out)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dimension:\n",
    "        # (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose:\n",
    "        # (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3) # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(\n",
    "            mask_bool,\n",
    "            -torch.inf\n",
    "        )\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / self.head_dim**0.5,\n",
    "            dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # Apply dropout to attention weights\n",
    "\n",
    "        # Compute context vectors for each head and\n",
    "        # Reshape back to (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        # optional projection\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ecaf38",
   "metadata": {},
   "source": [
    "There are lots of reshaping (`.view`) and transposing (`.transpose`) operations involved in this implementation. The idea is to combine the weight matrices for all heads into single large weight matrices and then split the results into multiple heads after the linear transformations. This approach reduces the number of separate matrix multiplications, which can be more efficient in terms of computation and memory usage. The `MultiHeadAttention` class implements the same concept as the `MultiHeadAttentionWrapper` but in a more efficient manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857946c5",
   "metadata": {},
   "source": [
    "Compared to the `MultiHeadAttentionWrapper` where we stacked multiple `CausalAttention` layers, the `MultiHeadAttention` starts with a multi-head layer and then internally splits this layer into individual attention heads for processing.\n",
    "\n",
    "The splitting of the query, key, and value tensors is done using tensor reshaping and transposing operations (`.view` and `.transpose`).\n",
    "\n",
    "The key operation is to split the `d_out` dimension into `num_heads` and `head_dim`, where `head_dim = d_out / num_heads`. \n",
    "\n",
    "The tensors are then transposed to bring the `num_heads` dimension before the `num_tokens` dimension, resulting in a shape of `(batch_size, num_heads, num_tokens, head_dim)`. This arrangement allows each head to process its respective portion of the data independently, and correctly aligning the queries, keys, and values across the different heads and performing batched matrix multiplications for efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d09dc6",
   "metadata": {},
   "source": [
    "To demo the batched maxtrix multiplication, suppose we have the following tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c46826d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor a shape: torch.Size([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4)\n",
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "\n",
    "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "print(\"Tensor a shape:\", a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2298413d",
   "metadata": {},
   "source": [
    "Now we perform a batched matrix multiplication between the tensor `a` itself and a view of the tensor where we transposed the last two dimensions, `num_tokens` and `head_dim`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "daa21047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched matrix multiplication of a and a transposed:\n",
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Batched matrix multiplication of a and a transposed:\")\n",
    "print(a @ a.transpose(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf049ad7",
   "metadata": {},
   "source": [
    "In this case, the batched matrix multiplication in PyTorch handles the 4-dimensional input tensor so that the multiplication is carried out between the two last dimensions `(num_tokens, head_dim)` and then repeated for the individual heads.\n",
    "\n",
    "For example, the preceding becomes a more compact way to compute the matrix multiplication for each head separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d39b478f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head matrix multiplication result:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head matrix multiplication result:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n"
     ]
    }
   ],
   "source": [
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head matrix multiplication result:\\n\", first_res)\n",
    "\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head matrix multiplication result:\\n\", second_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af2522ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "# This is exactly equivalent to the batched matrix multiplication above.\n",
    "print(a @ a.transpose(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebced202",
   "metadata": {},
   "source": [
    "Continuing with `MultiHeadAttention`, after computing the attention weights and applying them to the value vectors for each head, we need to combine the outputs from all heads back into a single tensor with shape `(batch_size, num_tokens, d_out)`. \n",
    "\n",
    "Additionally, we added an output projection layer `self.out_proj` to the `MultiHeadAttention` class after combining the heads. This output projection is not strictly necessary but it is commonly used in many LLM architectures to further transform the combined output of the multi-head attention mechanism. This transformation can help in capturing more complex relationships and interactions between the different heads' outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a644e1",
   "metadata": {},
   "source": [
    "Testing the `MultiHeadAttention` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0337bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vectors from MultiHeadAttention:\n",
      " tensor([[[-0.0111,  0.6056],\n",
      "         [ 0.0435,  0.5950],\n",
      "         [ 0.0630,  0.5891],\n",
      "         [ 0.0426,  0.5836],\n",
      "         [ 0.0496,  0.5591],\n",
      "         [ 0.0353,  0.5700]],\n",
      "\n",
      "        [[-0.0111,  0.6056],\n",
      "         [ 0.0435,  0.5950],\n",
      "         [ 0.0630,  0.5891],\n",
      "         [ 0.0426,  0.5836],\n",
      "         [ 0.0496,  0.5591],\n",
      "         [ 0.0353,  0.5700]]], grad_fn=<ViewBackward0>)\n",
      "Context vectors shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape # batch size, number of tokens, input embedding size\n",
    "d_out = 2\n",
    "\n",
    "mha = MultiHeadAttention(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    context_length=context_length,\n",
    "    dropout=0.0,\n",
    "    num_heads=2\n",
    ")\n",
    "context_vecs = mha(batch)\n",
    "print(\"Context vectors from MultiHeadAttention:\\n\", context_vecs)\n",
    "print(\"Context vectors shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8dc8e",
   "metadata": {},
   "source": [
    "Now we can see the output dimension is directly controlled by the `d_out` parameter specified when creating the `MultiHeadAttention` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd213e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
