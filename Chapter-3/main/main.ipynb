{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80cb7369",
   "metadata": {},
   "source": [
    "# Chapter 3: Coding Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "469afd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "print(\"torch version:\", version(\"torch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281306c7",
   "metadata": {},
   "source": [
    "## 3.3 Attending to Different Parts of the Input with Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83989c2",
   "metadata": {},
   "source": [
    "### 3.3.1 A Simple Self-Attention Mechanism without Trainable Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff26a97",
   "metadata": {},
   "source": [
    "Assume we have an input sequence, denoted as *x*, consisting of *T* elements represented as *x(1)* to *x(T)*. For example, in natural language processing, these elements could represent words or tokens in a sentence but have already been transformed into embeddings. \n",
    "\n",
    "Consider an input text like \"Your journey starts with one step.\" Each element of the sequence, such as *x(1)*, corresponds to a *d*-dimensional embedding vector representing a specific token, such as \"Your\".\n",
    "\n",
    "In self-attention, our goal is to calculate context vectors *z(i)* for each element *x(i)* in the input sequence(, where *z* and *x* have the same dimension). A **context vector** can be interepreted as an enriched embedding that captures not only the information from the token itself but also relevant information from other tokens in the sequence.\n",
    "\n",
    "The concept of context vectors is essential in LLMs, which need to understand the relationships and relevance of words in a sentence to each other. A context vector *z(i)* is a weighted sum over the inputs *x(1)* to *x(T)*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3303cc4c",
   "metadata": {},
   "source": [
    "For example, suppose we focus on the embedding vector *x(2)*, which corresponds to the token \"journey\". This context vector *z(2)* is a weighted sum over all inputs *x(1)* to *x(T)* weighted with respect to the second input element *x(2)*. \n",
    "\n",
    "The attention weights are the weights that determine how much each of the input elements contributes to the weighted sum when computing *z(2)*.\n",
    "\n",
    "By convention, the unnormalized attention weights are referred to as **attention scores**, whereas the normalized weights are called **attention weights**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4941bc",
   "metadata": {},
   "source": [
    "Suppose we have the following input sentence that is already embedded in 3-dimensional vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9662b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "     [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "     [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "     [0.22, 0.58, 0.33], # with     (x^4)\n",
    "     [0.77, 0.25, 0.10], # one      (x^5)\n",
    "     [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322bf1e1",
   "metadata": {},
   "source": [
    "**Step 1**: compute unnormalized attention scores *w*.\n",
    "\n",
    "Suppose we use *x(2)* as the query *q(2)*, then we can compute the unnormalized attention scores via dot products:\n",
    "- $w(2,1) = x(1)q(2)^T$\n",
    "- $w(2,2) = x(2)q(2)^T$\n",
    "- $w(2,3) = x(3)q(2)^T$\n",
    "- ...\n",
    "- $w(2,T) = x(T)q(2)^T$\n",
    "\n",
    "where $w(2,1)$ tells us the input sequence element 2 was used as a query against input sequence element 1.\n",
    "\n",
    "Now we can compute the unnormalized attention scores by computing the dot product between the query *x(2)* and all other input tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccea7d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of attn_scores_2: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 2nd input token \"journey\"\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "print(\"Shape of attn_scores_2:\", attn_scores_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf8834a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores for the 2nd input token 'journey':\n",
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "for i, x_i in enumerate(inputs):\n",
    "    # dot product\n",
    "    # (transpose not necessary here since x_i and query are 1D tensors)\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(\"Attention scores for the 2nd input token 'journey':\")\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459e24a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product computed manually: tensor(0.9544)\n",
      "Dot product computed with torch.dot: tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "# This is totally equivalent:\n",
    "res = 0.\n",
    "\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx] * query[idx]\n",
    "\n",
    "print(\"Dot product computed manually:\", res)\n",
    "print(\"Dot product computed with torch.dot:\", torch.dot(inputs[0], query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f126392",
   "metadata": {},
   "source": [
    "**Step 2**: normalize the unnormalized attention scores so that they sum up to 1.\n",
    "\n",
    "This normalization is a convention that is useful for interpretation and maintaining training stability in an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "345ec4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights for the 2nd input token 'journey':\n",
      "tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights for the 2nd input token 'journey':\")\n",
    "print(attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d5f53e",
   "metadata": {},
   "source": [
    "However in practice, we use the **softmax** function for normalization, which is better at handling extreme values and has more desirable gradient properties during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dcf6e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights for the 2nd input token 'journey' using naive softmax:\n",
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights for the 2nd input token 'journey' using naive softmax:\")\n",
    "print(attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31f4dc1",
   "metadata": {},
   "source": [
    "Softmax ensures the attention weights are always positive and sum to 1, making the output interpretable as probabilities or relative importance.\n",
    "\n",
    "To avoid numerical instability (overflow/underflow) when computing the exponential of large or small values, we prefer the PyTorch built-in softmax function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2920b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights for the 2nd input token 'journey' using PyTorch softmax:\n",
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights for the 2nd input token 'journey' using PyTorch softmax:\")\n",
    "print(attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c050ba",
   "metadata": {},
   "source": [
    "**Step 3**: compute the context vector *z(2)* by multiplying the embedded input tokens *x(1)* to *x(T)* with the attention weights *a(2,1)* to *a(2,T)* and sum the resulting vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "430f0ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector for the 2nd input token 'journey':\n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # 2nd input token \"journey\"\n",
    "\n",
    "context_vector_2 = torch.zeros(query.shape) # initialize context vector\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vector_2 += attn_weights_2[i] * x_i\n",
    "\n",
    "print(\"Context vector for the 2nd input token 'journey':\")\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7d6aa3",
   "metadata": {},
   "source": [
    "### 3.3.2 Computing Attention Weights for All Input Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad0e2a",
   "metadata": {},
   "source": [
    "We have computed the attention weights and context vector for the 2nd input token \"journey\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adc0a795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights for the 2nd input token 'journey':\n",
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n"
     ]
    }
   ],
   "source": [
    "print(\"Attention weights for the 2nd input token 'journey':\")\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82904f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector for the 2nd input token 'journey':\n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"Context vector for the 2nd input token 'journey':\")\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0af29cf",
   "metadata": {},
   "source": [
    "Now we generalize this process to compute attention weights and context vectors for all input tokens in the sequence. This involves repeating the steps outlined above for each token in the input sequence, treating each token as a query in turn.\n",
    "\n",
    "In self-attention, this process starts with the calculation of attention scores, which are subsequently normalized to derive attention weights that sum to one. Later, these attention weights are used to generate the context vectors through a weighted sum of the input embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082e038f",
   "metadata": {},
   "source": [
    "**Step 1**: compute the unnormalized attention scores *w* for each input token as a query against all input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1924bc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores matrix:\n",
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty((inputs.shape[0], inputs.shape[0]))\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(\"Attention scores matrix:\")\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db0b81",
   "metadata": {},
   "source": [
    "Each element in the tensor represents the attention score between each pair of input tokens. We can achieve the same result using matrix multiplication for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ded95d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores matrix using matrix multiplication:\n",
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "\n",
    "print(\"Attention scores matrix using matrix multiplication:\")\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dbe274",
   "metadata": {},
   "source": [
    "**Step 2**: normalize each row so that the values in each row sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e05fc13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights matrix:\n",
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "print(\"Attention weights matrix:\")\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7789775a",
   "metadata": {},
   "source": [
    "We can verify that the rows all sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d99b340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of attention weights for the 2nd input token 'journey': tensor(1.)\n",
      "Sum of attention weights for all input tokens: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum(attn_weights[1])\n",
    "print(\"Sum of attention weights for the 2nd input token 'journey':\", row_2_sum)\n",
    "\n",
    "all_rows_sum = attn_weights.sum(dim=-1)\n",
    "print(\"Sum of attention weights for all input tokens:\", all_rows_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3c0565",
   "metadata": {},
   "source": [
    "**Step 3**: compute all context vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5e1dd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All context vectors:\n",
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "\n",
    "print(\"All context vectors:\")\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caea82ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous context vector for the 2nd input token 'journey':\n",
      "tensor([0.4419, 0.6515, 0.5683])\n",
      "Context vector from all_context_vecs for the 2nd input token 'journey':\n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"Previous context vector for the 2nd input token 'journey':\")\n",
    "print(context_vector_2)\n",
    "\n",
    "print(\"Context vector from all_context_vecs for the 2nd input token 'journey':\")\n",
    "print(all_context_vecs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f34f1",
   "metadata": {},
   "source": [
    "## 3.4 Implementing Self-Attention with Trainable Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec903d83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
