{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124cbe71",
   "metadata": {},
   "source": [
    "# Chapter 7: Fine-Tuning to Follow Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4dd140",
   "metadata": {},
   "source": [
    "## 7.2 Preparing a Dataset for Instruction Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53867ac1",
   "metadata": {},
   "source": [
    "We need to download and format the instruction dataset for instruction fine-tuning a pretrained LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c540d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "def download_and_load_file(filepath, url):\n",
    "    if not os.path.exists(filepath):\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        text_data = response.text\n",
    "\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(text_data)\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9854654c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "filepath = 'instruction-data.json'\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    "\n",
    "data = download_and_load_file(filepath, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2aa606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Evaluate the following phrase by transforming it into the spelling given.', 'input': 'freind --> friend', 'output': 'The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example entry:\\n\", data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676ed67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another example entry:\n",
      " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Another example entry:\\n\", data[999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78375d7e",
   "metadata": {},
   "source": [
    "Instruction fine-tuning involves training a model on a dataset where the input-output pairs are explicitly provided.\n",
    "\n",
    "There are two different example formats, referred to as *prompt styles*, used in the trianing of LLMs, such as Alpaca and Phi-3.\n",
    "- The *Alpaca* style uses a structured format with defined sections for instruction, input, and response:\n",
    "    ```\n",
    "    Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    Identify the correct spelling of the following word.\n",
    "\n",
    "    ### Input:\n",
    "    Ocassion\n",
    "\n",
    "    ### Response:\n",
    "    The correct spelling is 'Occasion'.\n",
    "    ```\n",
    "- The *Phi-3* style uses a conversational format with designated `<|user|>` and `<|assistant|>` tokens:\n",
    "    ```\n",
    "    <|user|> \n",
    "    Identify the correct spelling of the following word: 'Ocassion'\n",
    "\n",
    "    <|assistant|>\n",
    "    The correct spelling is 'Occasion'.\n",
    "    ```\n",
    "\n",
    "In this chapter, we will use the Alpaca-style format for instruction fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51054c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry['input'] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aae75567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Evaluate the following phrase by transforming it into the spelling given.\n",
      "\n",
      "### Input:\n",
      "freind --> friend\n",
      "\n",
      "### Response:\n",
      "The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[0])\n",
    "desired_output = f\"\\n\\n### Response:\\n{data[0]['output']}\"\n",
    "\n",
    "print(model_input + desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5e4dae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "# data without input field\n",
    "model_input = format_input(data[999])\n",
    "desired_output = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "\n",
    "print(model_input + desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edf4f013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 935\n",
      "Testing set size: 110\n",
      "Validation set size: 55\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset\n",
    "train_portion = int(len(data) * 0.85) # 85% for training\n",
    "test_portion = int(len(data) * 0.10)  # 10% for testing\n",
    "val_portion = len(data) - train_portion - test_portion  # 5% for validation\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]\n",
    "\n",
    "print(\"Training set size:\", len(train_data))\n",
    "print(\"Testing set size:\", len(test_data))\n",
    "print(\"Validation set size:\", len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f07566d",
   "metadata": {},
   "source": [
    "## 7.3 Organizing Data into Training Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac8a778",
   "metadata": {},
   "source": [
    "The next step is to build the training batches effectively. In previous chapters, the training batches were created automatically with the `DataLoader` class from PyTorch, which employs a default *collate function* to combine lists of samples into batches.\n",
    "\n",
    "A collate function is responsible for taking a list of data samples and merging them into a single batch that can be processed by the model during training.\n",
    "\n",
    "For our instruction fine-tuning task, we need to implement a custom collate function that can handle the specific structure of our instruction dataset.\n",
    "\n",
    "First, we need to define a `Dataset` class that can load and preprocess our instruction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaed8208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize text\n",
    "        self.encoded_texts = []\n",
    "\n",
    "        for entry in self.data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b321cc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "# Similarly as before, we use `gpt2` tokenizer and add special tokens\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19ca05",
   "metadata": {},
   "source": [
    "Then we will adopt a more sophisticated approach by developing a custom collate function that we can pass to the dataloader.\n",
    "\n",
    "This custom collate function pads the training examples in each batch to the same length while allowing different batches to have different lengths. This approach minimizes unnecessary padding by only extending sequences to match the longest sequence in each batch, not the longest sequence in the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "684b9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_1(\n",
    "        batch,\n",
    "        pad_token_id=50256,\n",
    "        device='cpu'\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    # and increase by 1 for padding\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    # Pad the prepare inputs\n",
    "    inputs_list = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Pad sequences to `batch_max_length`\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "\n",
    "        # Via `padded[:-1]`, we remove the extra padded tokens\n",
    "        # that have been added via the +1 setting in `batch_max_length`\n",
    "        # (the extra padding tokens will be relevant in later steps)\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_list.append(inputs)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_list).to(device)\n",
    "\n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "965af697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch before collation:\n",
      " [[0, 1, 2, 3, 4], [5, 6], [7, 8, 9]]\n",
      "Collated batch:\n",
      " tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "Collated batch shape: torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = [inputs_1, inputs_2, inputs_3]\n",
    "\n",
    "print(\"Batch before collation:\\n\", batch)\n",
    "collated_batch = custom_collate_draft_1(batch)\n",
    "print(\"Collated batch:\\n\", collated_batch)\n",
    "print(\"Collated batch shape:\", collated_batch.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306e2765",
   "metadata": {},
   "source": [
    "The `custom_collate_draft_1` function creates batches from lists of inputs. We also need to create batches with the target token IDs corresponding to the batch of input IDs. Similar to the process we used to pretrain an LLM, the target token IDs match the input token IDs but are shifted one position to the right, allowing the LLM to learn how to predict the next token in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14c5e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(\n",
    "        batch,\n",
    "        pad_token_id=50256,\n",
    "        device='cpu'\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    # Pad the prepare inputs\n",
    "    inputs_list, targets_list = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "\n",
    "        # Add an `<|endoftext|>` token at the end of each sequence\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to `batch_max_length`\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # Truncate the last token for inputs\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        # Shift one position to the right for targets\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        inputs_list.append(inputs)\n",
    "        targets_list.append(targets)\n",
    "\n",
    "    # Convert list of inputs/targets to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_list).to(device)\n",
    "    targets_tensor = torch.stack(targets_list).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab59afcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collated inputs:\n",
      " tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "Collated inputs shape: torch.Size([3, 5])\n",
      "Collated targets:\n",
      " tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n",
      "Collated targets shape: torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# Test the same batch with the new collate function\n",
    "collated_inputs, collated_targets = custom_collate_draft_2(batch)\n",
    "print(\"Collated inputs:\\n\", collated_inputs)\n",
    "print(\"Collated inputs shape:\", collated_inputs.shape)\n",
    "print(\"Collated targets:\\n\", collated_targets)\n",
    "print(\"Collated targets shape:\", collated_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c595d67",
   "metadata": {},
   "source": [
    "Next, we will assign a `-100` placeholder value to all padding tokens. Thhis special valuue allows us to exclude these padding tokens from contributing to the training loss calculation, ensuring that only meaningful data influences model learning.\n",
    "\n",
    "We only retain one `<|endoftext|>` token, ID `50256`, in the target list. Retaining it allows the LLM to learn when to generate an end-of-text token in response to instructions, which we use as an indicator that the generated resopnse is complete.\n",
    "\n",
    "In the following `custom_collate_fn` function, we modify and replace tokens with ID `50256` with `-100` in the target lists.\n",
    "\n",
    "In addition, we introduce an `allowed_max_length` parameter to optionally limit the length of the samples. This will be useful if we plan to work with our own dataset that exceed the 1024-token context size supported by the GPT-2 model we are using for instruction fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a44a1e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "        batch,\n",
    "        pad_token_id=50256,\n",
    "        ignore_index=-100,\n",
    "        allowed_max_length=None,\n",
    "        device='cpu'\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    # Pad the prepare inputs and targets\n",
    "    inputs_list, targets_list = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "\n",
    "        # Add an `<|endoftext|>` token at the end of each sequence\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to `batch_max_length`\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "\n",
    "        # Truncate the last token for inputs\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        # Shift one position to the right for targets\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        # NEW: Replace all but the first padding tokens in targets by `ignore_index`\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # NEW: Optionally truncate sequences to `allowed_max_length`\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_list.append(inputs)\n",
    "        targets_list.append(targets)\n",
    "\n",
    "    # Convert list of inputs/targets to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_list).to(device)\n",
    "    targets_tensor = torch.stack(targets_list).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9ef9c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collated inputs:\n",
      " tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "Collated inputs shape: torch.Size([3, 5])\n",
      "Collated targets:\n",
      " tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n",
      "Collated targets shape: torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# Test the same batch with the new collate function\n",
    "collated_inputs, collated_targets = custom_collate_fn(batch)\n",
    "print(\"Collated inputs:\\n\", collated_inputs)\n",
    "print(\"Collated inputs shape:\", collated_inputs.shape)\n",
    "print(\"Collated targets:\\n\", collated_targets)\n",
    "print(\"Collated targets shape:\", collated_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50330505",
   "metadata": {},
   "source": [
    "Consider the following exampple where each output logit corresponds to a potential token from the model's vocabulary. We can calculate the cross entropy loss during training when the model predicts a sequence of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22535e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1: 1.1269280910491943\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor(\n",
    "    [[-1., 1.],   # 1st training example\n",
    "     [-0.5, 1.5]] # 2nd training example\n",
    ")\n",
    "targets_1 = torch.tensor([0, 1])\n",
    "\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(\"Loss 1:\", loss_1.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7d3b4",
   "metadata": {},
   "source": [
    "If we add an additional token ID, it would affect the loss calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c4f1e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 2: 0.7935947775840759\n"
     ]
    }
   ],
   "source": [
    "logits_2 = torch.tensor(\n",
    "    [[-1., 1.],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]] # New 3rd training example\n",
    ")\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(\"Loss 2:\", loss_2.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed11815e",
   "metadata": {},
   "source": [
    "If we set the additional token ID to `-100`, it will be ignored in the loss calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebf02a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 3: 1.1269280910491943\n"
     ]
    }
   ],
   "source": [
    "# logits_3 starts the same\n",
    "logits_3 = torch.tensor(\n",
    "    [[-1., 1.],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]]\n",
    ")\n",
    "\n",
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_3, targets_3)\n",
    "print(\"Loss 3:\", loss_3.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c46b72",
   "metadata": {},
   "source": [
    "The resulting loss only considers the valid token predictions, effectively ignoring the padding token.\n",
    "\n",
    "By default, PyTorch has the `cross_entropy` setting `ignore_index=-100`, which means that any target token with the value `-100` will be excluded from the loss computation. Using `-100` `ignore_index`, we can ignore the additional end-of-text tokens in the batches that we used to pad the training examples to equal lengths.\n",
    "\n",
    "However, we do not want to ignore the first instance of the end-of-text token because it can help signal to the LLM when the response is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a7e94",
   "metadata": {},
   "source": [
    "## 7.4 Creating Dataloaders for an Instruction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e3f20b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
