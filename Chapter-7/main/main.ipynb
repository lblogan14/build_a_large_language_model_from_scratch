{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124cbe71",
   "metadata": {},
   "source": [
    "# Chapter 7: Fine-Tuning to Follow Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4dd140",
   "metadata": {},
   "source": [
    "## 7.2 Preparing a Dataset for Instruction Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53867ac1",
   "metadata": {},
   "source": [
    "We need to download and format the instruction dataset for instruction fine-tuning a pretrained LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c540d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "def download_and_load_file(filepath, url):\n",
    "    if not os.path.exists(filepath):\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        text_data = response.text\n",
    "\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(text_data)\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9854654c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "filepath = 'instruction-data.json'\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    "\n",
    "data = download_and_load_file(filepath, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2aa606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Evaluate the following phrase by transforming it into the spelling given.', 'input': 'freind --> friend', 'output': 'The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example entry:\\n\", data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676ed67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another example entry:\n",
      " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Another example entry:\\n\", data[999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78375d7e",
   "metadata": {},
   "source": [
    "Instruction fine-tuning involves training a model on a dataset where the input-output pairs are explicitly provided.\n",
    "\n",
    "There are two different example formats, referred to as *prompt styles*, used in the trianing of LLMs, such as Alpaca and Phi-3.\n",
    "- The *Alpaca* style uses a structured format with defined sections for instruction, input, and response:\n",
    "    ```\n",
    "    Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    Identify the correct spelling of the following word.\n",
    "\n",
    "    ### Input:\n",
    "    Ocassion\n",
    "\n",
    "    ### Response:\n",
    "    The correct spelling is 'Occasion'.\n",
    "    ```\n",
    "- The *Phi-3* style uses a conversational format with designated `<|user|>` and `<|assistant|>` tokens:\n",
    "    ```\n",
    "    <|user|> \n",
    "    Identify the correct spelling of the following word: 'Ocassion'\n",
    "\n",
    "    <|assistant|>\n",
    "    The correct spelling is 'Occasion'.\n",
    "    ```\n",
    "\n",
    "In this chapter, we will use the Alpaca-style format for instruction fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51054c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry['input'] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aae75567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Evaluate the following phrase by transforming it into the spelling given.\n",
      "\n",
      "### Input:\n",
      "freind --> friend\n",
      "\n",
      "### Response:\n",
      "The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[0])\n",
    "desired_output = f\"\\n\\n### Response:\\n{data[0]['output']}\"\n",
    "\n",
    "print(model_input + desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5e4dae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "# data without input field\n",
    "model_input = format_input(data[999])\n",
    "desired_output = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "\n",
    "print(model_input + desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edf4f013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 935\n",
      "Testing set size: 110\n",
      "Validation set size: 55\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset\n",
    "train_portion = int(len(data) * 0.85) # 85% for training\n",
    "test_portion = int(len(data) * 0.10)  # 10% for testing\n",
    "val_portion = len(data) - train_portion - test_portion  # 5% for validation\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]\n",
    "\n",
    "print(\"Training set size:\", len(train_data))\n",
    "print(\"Testing set size:\", len(test_data))\n",
    "print(\"Validation set size:\", len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f07566d",
   "metadata": {},
   "source": [
    "## 7.3 Organizing Data into Training Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac8a778",
   "metadata": {},
   "source": [
    "The next step is to build the training batches effectively. In previous chapters, the training batches were created automatically with the `DataLoader` class from PyTorch, which employs a default *collate function* to combine lists of samples into batches.\n",
    "\n",
    "A collate function is responsible for taking a list of data samples and merging them into a single batch that can be processed by the model during training.\n",
    "\n",
    "For our instruction fine-tuning task, we need to implement a custom collate function that can handle the specific structure of our instruction dataset.\n",
    "\n",
    "First, we need to define a `Dataset` class that can load and preprocess our instruction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaed8208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize text\n",
    "        self.encoded_texts = []\n",
    "\n",
    "        for entry in self.data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b321cc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "# Similarly as before, we use `gpt2` tokenizer and add special tokens\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19ca05",
   "metadata": {},
   "source": [
    "Then we will adopt a more sophisticated approach by developing a custom collate function that we can pass to the dataloader.\n",
    "\n",
    "This custom collate function pads the training examples in each batch to the same length while allowing different batches to have different lengths. This approach minimizes unnecessary padding by only extending sequences to match the longest sequence in each batch, not the longest sequence in the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "684b9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_1(\n",
    "        batch,\n",
    "        pad_token_id=50256,\n",
    "        device='cpu'\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    # and increase by 1 for padding\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    # Pad the prepare inputs\n",
    "    inputs_list = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Pad sequences to `batch_max_length`\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "\n",
    "        # Via `padded[:-1]`, we remove the extra padded tokens\n",
    "        # that have been added via the +1 setting in `batch_max_length`\n",
    "        # (the extra padding tokens will be relevant in later steps)\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_list.append(inputs)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_list).to(device)\n",
    "\n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "965af697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch before collation:\n",
      " [[0, 1, 2, 3, 4], [5, 6], [7, 8, 9]]\n",
      "Collated batch:\n",
      " tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "Collated batch shape: torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = [inputs_1, inputs_2, inputs_3]\n",
    "\n",
    "print(\"Batch before collation:\\n\", batch)\n",
    "collated_batch = custom_collate_draft_1(batch)\n",
    "print(\"Collated batch:\\n\", collated_batch)\n",
    "print(\"Collated batch shape:\", collated_batch.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306e2765",
   "metadata": {},
   "source": [
    "The `custom_collate_draft_1` function creates batches from lists of inputs. We also need to create batches with the target token IDs corresponding to the batch of input IDs. Similar to the process we used to pretrain an LLM, the target token IDs match the input token IDs but are shifted one position to the right, allowing the LLM to learn how to predict the next token in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14c5e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(\n",
    "        batch,\n",
    "        pad_token_id=50256,\n",
    "        device='cpu'\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    # Pad the prepare inputs\n",
    "    inputs_list, targets_list = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "\n",
    "        # Add an `<|endoftext|>` token at the end of each sequence\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to `batch_max_length`\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # Truncate the last token for inputs\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        # Shift one position to the right for targets\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        inputs_list.append(inputs)\n",
    "        targets_list.append(targets)\n",
    "\n",
    "    # Convert list of inputs/targets to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_list).to(device)\n",
    "    targets_tensor = torch.stack(targets_list).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab59afcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collated inputs:\n",
      " tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "Collated inputs shape: torch.Size([3, 5])\n",
      "Collated targets:\n",
      " tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n",
      "Collated targets shape: torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# Test the same batch with the new collate function\n",
    "collated_inputs, collated_targets = custom_collate_draft_2(batch)\n",
    "print(\"Collated inputs:\\n\", collated_inputs)\n",
    "print(\"Collated inputs shape:\", collated_inputs.shape)\n",
    "print(\"Collated targets:\\n\", collated_targets)\n",
    "print(\"Collated targets shape:\", collated_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c595d67",
   "metadata": {},
   "source": [
    "Next, we will assign a `-100` placeholder value to all padding tokens. Thhis special valuue allows us to exclude these padding tokens from contributing to the training loss calculation, ensuring that only meaningful data influences model learning.\n",
    "\n",
    "We only retain one `<|endoftext|>` token, ID `50256`, in the target list. Retaining it allows the LLM to learn when to generate an end-of-text token in response to instructions, which we use as an indicator that the generated resopnse is complete.\n",
    "\n",
    "In the following `custom_collate_fn` function, we modify and replace tokens with ID `50256` with `-100` in the target lists.\n",
    "\n",
    "In addition, we introduce an `allowed_max_length` parameter to optionally limit the length of the samples. This will be useful if we plan to work with our own dataset that exceed the 1024-token context size supported by the GPT-2 model we are using for instruction fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a44a1e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "        batch,\n",
    "        pad_token_id=50256,\n",
    "        ignore_index=-100,\n",
    "        allowed_max_length=None,\n",
    "        device='cpu'\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    # Pad the prepare inputs and targets\n",
    "    inputs_list, targets_list = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "\n",
    "        # Add an `<|endoftext|>` token at the end of each sequence\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to `batch_max_length`\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "\n",
    "        # Truncate the last token for inputs\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        # Shift one position to the right for targets\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        # NEW: Replace all but the first padding tokens in targets by `ignore_index`\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # NEW: Optionally truncate sequences to `allowed_max_length`\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_list.append(inputs)\n",
    "        targets_list.append(targets)\n",
    "\n",
    "    # Convert list of inputs/targets to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_list).to(device)\n",
    "    targets_tensor = torch.stack(targets_list).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9ef9c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collated inputs:\n",
      " tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "Collated inputs shape: torch.Size([3, 5])\n",
      "Collated targets:\n",
      " tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n",
      "Collated targets shape: torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# Test the same batch with the new collate function\n",
    "collated_inputs, collated_targets = custom_collate_fn(batch)\n",
    "print(\"Collated inputs:\\n\", collated_inputs)\n",
    "print(\"Collated inputs shape:\", collated_inputs.shape)\n",
    "print(\"Collated targets:\\n\", collated_targets)\n",
    "print(\"Collated targets shape:\", collated_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50330505",
   "metadata": {},
   "source": [
    "Consider the following exampple where each output logit corresponds to a potential token from the model's vocabulary. We can calculate the cross entropy loss during training when the model predicts a sequence of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22535e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1: 1.1269280910491943\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor(\n",
    "    [[-1., 1.],   # 1st training example\n",
    "     [-0.5, 1.5]] # 2nd training example\n",
    ")\n",
    "targets_1 = torch.tensor([0, 1])\n",
    "\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(\"Loss 1:\", loss_1.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7d3b4",
   "metadata": {},
   "source": [
    "If we add an additional token ID, it would affect the loss calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c4f1e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 2: 0.7935947775840759\n"
     ]
    }
   ],
   "source": [
    "logits_2 = torch.tensor(\n",
    "    [[-1., 1.],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]] # New 3rd training example\n",
    ")\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(\"Loss 2:\", loss_2.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed11815e",
   "metadata": {},
   "source": [
    "If we set the additional token ID to `-100`, it will be ignored in the loss calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebf02a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 3: 1.1269280910491943\n"
     ]
    }
   ],
   "source": [
    "# logits_3 starts the same\n",
    "logits_3 = torch.tensor(\n",
    "    [[-1., 1.],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]]\n",
    ")\n",
    "\n",
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_3, targets_3)\n",
    "print(\"Loss 3:\", loss_3.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c46b72",
   "metadata": {},
   "source": [
    "The resulting loss only considers the valid token predictions, effectively ignoring the padding token.\n",
    "\n",
    "By default, PyTorch has the `cross_entropy` setting `ignore_index=-100`, which means that any target token with the value `-100` will be excluded from the loss computation. Using `-100` `ignore_index`, we can ignore the additional end-of-text tokens in the batches that we used to pad the training examples to equal lengths.\n",
    "\n",
    "However, we do not want to ignore the first instance of the end-of-text token because it can help signal to the LLM when the response is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a7e94",
   "metadata": {},
   "source": [
    "## 7.4 Creating Dataloaders for an Instruction Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d690a0f",
   "metadata": {},
   "source": [
    "The `custom_collate_fn` function includes a `device` parameter that moves the input and target tensors to a specified device, such as a GPU, for efficient training.\n",
    "\n",
    "Previously we moved the data onto the target device in the main training loopp. Having this as part of the collate function allows us to transfer the data as a background processs outside the training loop, preventing it from blocking the GPU during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98e3f20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
    "    if (major, minor) >= (2, 9):\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3e659e",
   "metadata": {},
   "source": [
    "To reuse the chosen device setting in `custom_collate_fn`, we can use the `partial` function from the `functools` module to create a new version of the collate function with the device parameter set to the desired device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1424aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "custom_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b91caf",
   "metadata": {},
   "source": [
    "Next, we set up the dataloader as we did previously, but this time we will use our customized collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb9807d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abc42183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Inputs shape: torch.Size([8, 59])\n",
      "Targets shape: torch.Size([8, 59])\n",
      "Inputs shape: torch.Size([8, 65])\n",
      "Targets shape: torch.Size([8, 65])\n",
      "Inputs shape: torch.Size([8, 69])\n",
      "Targets shape: torch.Size([8, 69])\n",
      "Inputs shape: torch.Size([8, 73])\n",
      "Targets shape: torch.Size([8, 73])\n",
      "Inputs shape: torch.Size([8, 62])\n",
      "Targets shape: torch.Size([8, 62])\n",
      "Inputs shape: torch.Size([8, 60])\n",
      "Targets shape: torch.Size([8, 60])\n",
      "Inputs shape: torch.Size([8, 71])\n",
      "Targets shape: torch.Size([8, 71])\n",
      "Inputs shape: torch.Size([8, 60])\n",
      "Targets shape: torch.Size([8, 60])\n",
      "Inputs shape: torch.Size([8, 80])\n",
      "Targets shape: torch.Size([8, 80])\n",
      "Inputs shape: torch.Size([8, 76])\n",
      "Targets shape: torch.Size([8, 76])\n",
      "Inputs shape: torch.Size([8, 74])\n",
      "Targets shape: torch.Size([8, 74])\n",
      "Inputs shape: torch.Size([8, 62])\n",
      "Targets shape: torch.Size([8, 62])\n",
      "Inputs shape: torch.Size([8, 68])\n",
      "Targets shape: torch.Size([8, 68])\n",
      "Inputs shape: torch.Size([8, 58])\n",
      "Targets shape: torch.Size([8, 58])\n",
      "Inputs shape: torch.Size([8, 83])\n",
      "Targets shape: torch.Size([8, 83])\n",
      "Inputs shape: torch.Size([8, 91])\n",
      "Targets shape: torch.Size([8, 91])\n",
      "Inputs shape: torch.Size([8, 65])\n",
      "Targets shape: torch.Size([8, 65])\n",
      "Inputs shape: torch.Size([8, 71])\n",
      "Targets shape: torch.Size([8, 71])\n",
      "Inputs shape: torch.Size([8, 67])\n",
      "Targets shape: torch.Size([8, 67])\n",
      "Inputs shape: torch.Size([8, 77])\n",
      "Targets shape: torch.Size([8, 77])\n",
      "Inputs shape: torch.Size([8, 79])\n",
      "Targets shape: torch.Size([8, 79])\n",
      "Inputs shape: torch.Size([8, 71])\n",
      "Targets shape: torch.Size([8, 71])\n",
      "Inputs shape: torch.Size([8, 68])\n",
      "Targets shape: torch.Size([8, 68])\n",
      "Inputs shape: torch.Size([8, 70])\n",
      "Targets shape: torch.Size([8, 70])\n",
      "Inputs shape: torch.Size([8, 75])\n",
      "Targets shape: torch.Size([8, 75])\n",
      "Inputs shape: torch.Size([8, 74])\n",
      "Targets shape: torch.Size([8, 74])\n",
      "Inputs shape: torch.Size([8, 82])\n",
      "Targets shape: torch.Size([8, 82])\n",
      "Inputs shape: torch.Size([8, 68])\n",
      "Targets shape: torch.Size([8, 68])\n",
      "Inputs shape: torch.Size([8, 59])\n",
      "Targets shape: torch.Size([8, 59])\n",
      "Inputs shape: torch.Size([8, 67])\n",
      "Targets shape: torch.Size([8, 67])\n",
      "Inputs shape: torch.Size([8, 67])\n",
      "Targets shape: torch.Size([8, 67])\n",
      "Inputs shape: torch.Size([8, 78])\n",
      "Targets shape: torch.Size([8, 78])\n",
      "Inputs shape: torch.Size([8, 77])\n",
      "Targets shape: torch.Size([8, 77])\n",
      "Inputs shape: torch.Size([8, 68])\n",
      "Targets shape: torch.Size([8, 68])\n",
      "Inputs shape: torch.Size([8, 89])\n",
      "Targets shape: torch.Size([8, 89])\n",
      "Inputs shape: torch.Size([8, 62])\n",
      "Targets shape: torch.Size([8, 62])\n",
      "Inputs shape: torch.Size([8, 63])\n",
      "Targets shape: torch.Size([8, 63])\n",
      "Inputs shape: torch.Size([8, 63])\n",
      "Targets shape: torch.Size([8, 63])\n",
      "Inputs shape: torch.Size([8, 69])\n",
      "Targets shape: torch.Size([8, 69])\n",
      "Inputs shape: torch.Size([8, 88])\n",
      "Targets shape: torch.Size([8, 88])\n",
      "Inputs shape: torch.Size([8, 64])\n",
      "Targets shape: torch.Size([8, 64])\n",
      "Inputs shape: torch.Size([8, 65])\n",
      "Targets shape: torch.Size([8, 65])\n",
      "Inputs shape: torch.Size([8, 67])\n",
      "Targets shape: torch.Size([8, 67])\n",
      "Inputs shape: torch.Size([8, 65])\n",
      "Targets shape: torch.Size([8, 65])\n",
      "Inputs shape: torch.Size([8, 75])\n",
      "Targets shape: torch.Size([8, 75])\n",
      "Inputs shape: torch.Size([8, 59])\n",
      "Targets shape: torch.Size([8, 59])\n",
      "Inputs shape: torch.Size([8, 61])\n",
      "Targets shape: torch.Size([8, 61])\n",
      "Inputs shape: torch.Size([8, 76])\n",
      "Targets shape: torch.Size([8, 76])\n",
      "Inputs shape: torch.Size([8, 74])\n",
      "Targets shape: torch.Size([8, 74])\n",
      "Inputs shape: torch.Size([8, 64])\n",
      "Targets shape: torch.Size([8, 64])\n",
      "Inputs shape: torch.Size([8, 68])\n",
      "Targets shape: torch.Size([8, 68])\n",
      "Inputs shape: torch.Size([8, 83])\n",
      "Targets shape: torch.Size([8, 83])\n",
      "Inputs shape: torch.Size([8, 58])\n",
      "Targets shape: torch.Size([8, 58])\n",
      "Inputs shape: torch.Size([8, 57])\n",
      "Targets shape: torch.Size([8, 57])\n",
      "Inputs shape: torch.Size([8, 61])\n",
      "Targets shape: torch.Size([8, 61])\n",
      "Inputs shape: torch.Size([8, 77])\n",
      "Targets shape: torch.Size([8, 77])\n",
      "Inputs shape: torch.Size([8, 63])\n",
      "Targets shape: torch.Size([8, 63])\n",
      "Inputs shape: torch.Size([8, 67])\n",
      "Targets shape: torch.Size([8, 67])\n",
      "Inputs shape: torch.Size([8, 72])\n",
      "Targets shape: torch.Size([8, 72])\n",
      "Inputs shape: torch.Size([8, 65])\n",
      "Targets shape: torch.Size([8, 65])\n",
      "Inputs shape: torch.Size([8, 64])\n",
      "Targets shape: torch.Size([8, 64])\n",
      "Inputs shape: torch.Size([8, 66])\n",
      "Targets shape: torch.Size([8, 66])\n",
      "Inputs shape: torch.Size([8, 68])\n",
      "Targets shape: torch.Size([8, 68])\n",
      "Inputs shape: torch.Size([8, 64])\n",
      "Targets shape: torch.Size([8, 64])\n",
      "Inputs shape: torch.Size([8, 67])\n",
      "Targets shape: torch.Size([8, 67])\n",
      "Inputs shape: torch.Size([8, 69])\n",
      "Targets shape: torch.Size([8, 69])\n",
      "Inputs shape: torch.Size([8, 59])\n",
      "Targets shape: torch.Size([8, 59])\n",
      "Inputs shape: torch.Size([8, 71])\n",
      "Targets shape: torch.Size([8, 71])\n",
      "Inputs shape: torch.Size([8, 75])\n",
      "Targets shape: torch.Size([8, 75])\n",
      "Inputs shape: torch.Size([8, 76])\n",
      "Targets shape: torch.Size([8, 76])\n",
      "Inputs shape: torch.Size([8, 74])\n",
      "Targets shape: torch.Size([8, 74])\n",
      "Inputs shape: torch.Size([8, 65])\n",
      "Targets shape: torch.Size([8, 65])\n",
      "Inputs shape: torch.Size([8, 83])\n",
      "Targets shape: torch.Size([8, 83])\n",
      "Inputs shape: torch.Size([8, 70])\n",
      "Targets shape: torch.Size([8, 70])\n",
      "Inputs shape: torch.Size([8, 72])\n",
      "Targets shape: torch.Size([8, 72])\n",
      "Inputs shape: torch.Size([8, 81])\n",
      "Targets shape: torch.Size([8, 81])\n",
      "Inputs shape: torch.Size([8, 91])\n",
      "Targets shape: torch.Size([8, 91])\n",
      "Inputs shape: torch.Size([8, 83])\n",
      "Targets shape: torch.Size([8, 83])\n",
      "Inputs shape: torch.Size([8, 65])\n",
      "Targets shape: torch.Size([8, 65])\n",
      "Inputs shape: torch.Size([8, 68])\n",
      "Targets shape: torch.Size([8, 68])\n",
      "Inputs shape: torch.Size([8, 83])\n",
      "Targets shape: torch.Size([8, 83])\n",
      "Inputs shape: torch.Size([8, 63])\n",
      "Targets shape: torch.Size([8, 63])\n",
      "Inputs shape: torch.Size([8, 70])\n",
      "Targets shape: torch.Size([8, 70])\n",
      "Inputs shape: torch.Size([8, 60])\n",
      "Targets shape: torch.Size([8, 60])\n",
      "Inputs shape: torch.Size([8, 74])\n",
      "Targets shape: torch.Size([8, 74])\n",
      "Inputs shape: torch.Size([8, 73])\n",
      "Targets shape: torch.Size([8, 73])\n",
      "Inputs shape: torch.Size([8, 71])\n",
      "Targets shape: torch.Size([8, 71])\n",
      "Inputs shape: torch.Size([8, 83])\n",
      "Targets shape: torch.Size([8, 83])\n",
      "Inputs shape: torch.Size([8, 63])\n",
      "Targets shape: torch.Size([8, 63])\n",
      "Inputs shape: torch.Size([8, 69])\n",
      "Targets shape: torch.Size([8, 69])\n",
      "Inputs shape: torch.Size([8, 64])\n",
      "Targets shape: torch.Size([8, 64])\n",
      "Inputs shape: torch.Size([8, 66])\n",
      "Targets shape: torch.Size([8, 66])\n",
      "Inputs shape: torch.Size([8, 71])\n",
      "Targets shape: torch.Size([8, 71])\n",
      "Inputs shape: torch.Size([8, 80])\n",
      "Targets shape: torch.Size([8, 80])\n",
      "Inputs shape: torch.Size([8, 64])\n",
      "Targets shape: torch.Size([8, 64])\n",
      "Inputs shape: torch.Size([8, 71])\n",
      "Targets shape: torch.Size([8, 71])\n",
      "Inputs shape: torch.Size([8, 68])\n",
      "Targets shape: torch.Size([8, 68])\n",
      "Inputs shape: torch.Size([8, 72])\n",
      "Targets shape: torch.Size([8, 72])\n",
      "Inputs shape: torch.Size([8, 73])\n",
      "Targets shape: torch.Size([8, 73])\n",
      "Inputs shape: torch.Size([8, 65])\n",
      "Targets shape: torch.Size([8, 65])\n",
      "Inputs shape: torch.Size([8, 75])\n",
      "Targets shape: torch.Size([8, 75])\n",
      "Inputs shape: torch.Size([8, 80])\n",
      "Targets shape: torch.Size([8, 80])\n",
      "Inputs shape: torch.Size([8, 66])\n",
      "Targets shape: torch.Size([8, 66])\n",
      "Inputs shape: torch.Size([8, 70])\n",
      "Targets shape: torch.Size([8, 70])\n",
      "Inputs shape: torch.Size([8, 68])\n",
      "Targets shape: torch.Size([8, 68])\n",
      "Inputs shape: torch.Size([8, 67])\n",
      "Targets shape: torch.Size([8, 67])\n",
      "Inputs shape: torch.Size([8, 91])\n",
      "Targets shape: torch.Size([8, 91])\n",
      "Inputs shape: torch.Size([8, 72])\n",
      "Targets shape: torch.Size([8, 72])\n",
      "Inputs shape: torch.Size([8, 56])\n",
      "Targets shape: torch.Size([8, 56])\n",
      "Inputs shape: torch.Size([8, 70])\n",
      "Targets shape: torch.Size([8, 70])\n",
      "Inputs shape: torch.Size([8, 87])\n",
      "Targets shape: torch.Size([8, 87])\n",
      "Inputs shape: torch.Size([8, 69])\n",
      "Targets shape: torch.Size([8, 69])\n",
      "Inputs shape: torch.Size([8, 68])\n",
      "Targets shape: torch.Size([8, 68])\n",
      "Inputs shape: torch.Size([8, 67])\n",
      "Targets shape: torch.Size([8, 67])\n",
      "Inputs shape: torch.Size([8, 67])\n",
      "Targets shape: torch.Size([8, 67])\n",
      "Inputs shape: torch.Size([8, 66])\n",
      "Targets shape: torch.Size([8, 66])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(\"Inputs shape:\", inputs.shape)\n",
    "    print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27402190",
   "metadata": {},
   "source": [
    "All batches have a batch size of 8 but a different sequence length depending on the longest sequence in each batch as expected.\n",
    "\n",
    "We also need to check the `<|endoftext|>` token in the target batches to ensure that only one instance of this token is present in each target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56ea5570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example input sequence:\n",
      "tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
      "        21017, 46486,    25,   198, 15946,   485,   257,  1573,   326,  9529,\n",
      "        22009,   351,   366,  6651,   526,   198,   198, 21017, 18261,    25,\n",
      "          198,    32,  1573,   326,  9529, 22009,   351,   366,  6651,     1,\n",
      "          318,   366,  9496,   526, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256])\n",
      "Example target sequence:\n",
      "tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,\n",
      "         2882,   326, 20431, 32543,   262,  2581,    13,   198,   198, 21017,\n",
      "        46486,    25,   198, 15946,   485,   257,  1573,   326,  9529, 22009,\n",
      "          351,   366,  6651,   526,   198,   198, 21017, 18261,    25,   198,\n",
      "           32,  1573,   326,  9529, 22009,   351,   366,  6651,     1,   318,\n",
      "          366,  9496,   526, 50256,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "print(\"Example input sequence:\")\n",
    "print(inputs[0])\n",
    "\n",
    "print(\"Example target sequence:\")\n",
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a849d42c",
   "metadata": {},
   "source": [
    "## 7.5 Loading a Pretrained LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4868b4e",
   "metadata": {},
   "source": [
    "We need to load the medium-sized GPT-2 model with 355M parameters as our base LLM for instruction fine-tuning because the small GPT-2 model with 124M parameters may not have sufficient capacity to effectively learn and generalize from the instruction dataset. The storage of the medium-sized model is approximately 1.5GB, which is about three times larger than the small model's size of approximately 500MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d4558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms_from_scratch.ch04 import GPTModel\n",
    "from llms_from_scratch.ch05 import download_and_load_gpt2, load_weights_into_gpt\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0777b3",
   "metadata": {},
   "source": [
    "We need to test the performance of the medium-sized GPT-2 model on our instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "input_text = format_input(data[0])\n",
    "print(\"Sample input text:\\n\", input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2857f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms_from_scratch.ch05 import generate, text_to_token_ids, token_ids_to_text\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=100,\n",
    "    context_size=BASE_CONFIG['context_length'],\n",
    "    eos_id=50256\n",
    ")\n",
    "\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ea60c",
   "metadata": {},
   "source": [
    "The `generate` function returns the combined input and output text, which is convenient in the previous section for creating legible text.\n",
    "\n",
    "Now we need to separate the generated output text from the input prompt to evaluate the model's responses accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb7609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "print(\"Generated response:\\n\", response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae516c0",
   "metadata": {},
   "source": [
    "## 7.6 Fine-Tuning the LLM on Instruction Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092be97",
   "metadata": {},
   "source": [
    "For the fine-tuning process, we will reuse the loss calculation and training functions implemented in Chapter 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce3acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms_from_scratch.ch05 import calc_loss_loader, train_model_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44271878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate initial losses\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "print(f\"Initial train loss: {train_loss:.4f}\")\n",
    "print(f\"Initial val loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e9503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=format_input(val_data[0]),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93602589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms_from_scratch.ch05 import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811b808c",
   "metadata": {},
   "source": [
    "## 7.7 Extracting and Saving Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6aff1",
   "metadata": {},
   "source": [
    "Now we are ready to evaluate the fine-tuned LLM on the instruction dataset by extracting the model-generated responses for each input in the test dataset and collecting them for manual analysis, and then evaluating the LLM to quantify the quality of the responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99343bf0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
