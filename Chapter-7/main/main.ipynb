{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124cbe71",
   "metadata": {},
   "source": [
    "# Chapter 7: Fine-Tuning to Follow Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4dd140",
   "metadata": {},
   "source": [
    "## 7.2 Preparing a Dataset for Instruction Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53867ac1",
   "metadata": {},
   "source": [
    "We need to download and format the instruction dataset for instruction fine-tuning a pretrained LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c540d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "def download_and_load_file(filepath, url):\n",
    "    if not os.path.exists(filepath):\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        text_data = response.text\n",
    "\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(text_data)\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9854654c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "filepath = 'instruction-data.json'\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    "\n",
    "data = download_and_load_file(filepath, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2aa606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Evaluate the following phrase by transforming it into the spelling given.', 'input': 'freind --> friend', 'output': 'The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example entry:\\n\", data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676ed67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another example entry:\n",
      " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Another example entry:\\n\", data[999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78375d7e",
   "metadata": {},
   "source": [
    "Instruction fine-tuning involves training a model on a dataset where the input-output pairs are explicitly provided.\n",
    "\n",
    "There are two different example formats, referred to as *prompt styles*, used in the trianing of LLMs, such as Alpaca and Phi-3.\n",
    "- The *Alpaca* style uses a structured format with defined sections for instruction, input, and response:\n",
    "    ```\n",
    "    Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    Identify the correct spelling of the following word.\n",
    "\n",
    "    ### Input:\n",
    "    Ocassion\n",
    "\n",
    "    ### Response:\n",
    "    The correct spelling is 'Occasion'.\n",
    "    ```\n",
    "- The *Phi-3* style uses a conversational format with designated `<|user|>` and `<|assistant|>` tokens:\n",
    "    ```\n",
    "    <|user|> \n",
    "    Identify the correct spelling of the following word: 'Ocassion'\n",
    "\n",
    "    <|assistant|>\n",
    "    The correct spelling is 'Occasion'.\n",
    "    ```\n",
    "\n",
    "In this chapter, we will use the Alpaca-style format for instruction fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51054c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry['input'] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aae75567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Evaluate the following phrase by transforming it into the spelling given.\n",
      "\n",
      "### Input:\n",
      "freind --> friend\n",
      "\n",
      "### Response:\n",
      "The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[0])\n",
    "desired_output = f\"\\n\\n### Response:\\n{data[0]['output']}\"\n",
    "\n",
    "print(model_input + desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5e4dae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "# data without input field\n",
    "model_input = format_input(data[999])\n",
    "desired_output = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "\n",
    "print(model_input + desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edf4f013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 935\n",
      "Testing set size: 110\n",
      "Validation set size: 55\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset\n",
    "train_portion = int(len(data) * 0.85) # 85% for training\n",
    "test_portion = int(len(data) * 0.10)  # 10% for testing\n",
    "val_portion = len(data) - train_portion - test_portion  # 5% for validation\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]\n",
    "\n",
    "print(\"Training set size:\", len(train_data))\n",
    "print(\"Testing set size:\", len(test_data))\n",
    "print(\"Validation set size:\", len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f07566d",
   "metadata": {},
   "source": [
    "## 7.3 Organizing Data into Training Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac8a778",
   "metadata": {},
   "source": [
    "The next step is to build the training batches effectively. In previous chapters, the training batches were created automatically with the `DataLoader` class from PyTorch, which employs a default *collate function* to combine lists of samples into batches.\n",
    "\n",
    "A collate function is responsible for taking a list of data samples and merging them into a single batch that can be processed by the model during training.\n",
    "\n",
    "For our instruction fine-tuning task, we need to implement a custom collate function that can handle the specific structure of our instruction dataset.\n",
    "\n",
    "First, we need to define a `Dataset` class that can load and preprocess our instruction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaed8208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize text\n",
    "        self.encoded_texts = []\n",
    "\n",
    "        for entry in self.data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b321cc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "# Similarly as before, we use `gpt2` tokenizer and add special tokens\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19ca05",
   "metadata": {},
   "source": [
    "Then we will adopt a more sophisticated approach by developing a custom collate function that we can pass to the dataloader.\n",
    "\n",
    "This custom collate function pads the training examples in each batch to the same length while allowing different batches to have different lengths. This approach minimizes unnecessary padding by only extending sequences to match the longest sequence in each batch, not the longest sequence in the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "684b9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_1(\n",
    "        batch,\n",
    "        pad_token_id=50256,\n",
    "        device='cpu'\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    # and increase by 1 for padding\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    # Pad the prepare inputs\n",
    "    inputs_list = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Pad sequences to `batch_max_length`\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "\n",
    "        # Via `padded[:-1]`, we remove the extra padded tokens\n",
    "        # that have been added via the +1 setting in `batch_max_length`\n",
    "        # (the extra padding tokens will be relevant in later steps)\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_list.append(inputs)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_list).to(device)\n",
    "\n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "965af697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch before collation:\n",
      " [[0, 1, 2, 3, 4], [5, 6], [7, 8, 9]]\n",
      "Collated batch:\n",
      " tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "Collated batch shape: torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = [inputs_1, inputs_2, inputs_3]\n",
    "\n",
    "print(\"Batch before collation:\\n\", batch)\n",
    "collated_batch = custom_collate_draft_1(batch)\n",
    "print(\"Collated batch:\\n\", collated_batch)\n",
    "print(\"Collated batch shape:\", collated_batch.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306e2765",
   "metadata": {},
   "source": [
    "The `custom_collate_draft_1` function creates batches from lists of inputs. We also need to create batches with the target token IDs corresponding to the batch of input IDs. Similar to the process we used to pretrain an LLM, the target token IDs match the input token IDs but are shifted one position to the right, allowing the LLM to learn how to predict the next token in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14c5e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(\n",
    "        batch,\n",
    "        pad_token_id=50256,\n",
    "        device='cpu'\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    # Pad the prepare inputs\n",
    "    inputs_list, targets_list = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "\n",
    "        # Add an `<|endoftext|>` token at the end of each sequence\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to `batch_max_length`\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # Truncate the last token for inputs\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        # Shift one position to the right for targets\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        inputs_list.append(inputs)\n",
    "        targets_list.append(targets)\n",
    "\n",
    "    # Convert list of inputs/targets to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_list).to(device)\n",
    "    targets_tensor = torch.stack(targets_list).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab59afcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collated inputs:\n",
      " tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "Collated inputs shape: torch.Size([3, 5])\n",
      "Collated targets:\n",
      " tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n",
      "Collated targets shape: torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# Test the same batch with the new collate function\n",
    "collated_inputs, collated_targets = custom_collate_draft_2(batch)\n",
    "print(\"Collated inputs:\\n\", collated_inputs)\n",
    "print(\"Collated inputs shape:\", collated_inputs.shape)\n",
    "print(\"Collated targets:\\n\", collated_targets)\n",
    "print(\"Collated targets shape:\", collated_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c595d67",
   "metadata": {},
   "source": [
    "Next, we will assign a `-100` placeholder value to all padding tokens. Thhis special valuue allows us to exclude these padding tokens from contributing to the training loss calculation, ensuring that only meaningful data influences model learning.\n",
    "\n",
    "We only retain one `<|endoftext|>` token, ID `50256`, in the target list. Retaining it allows the LLM to learn when to generate an end-of-text token in response to instructions, which we use as an indicator that the generated resopnse is complete.\n",
    "\n",
    "In the following `custom_collate_fn` function, we modify and replace tokens with ID `50256` with `-100` in the target lists.\n",
    "\n",
    "In addition, we introduce an `allowed_max_length` parameter to optionally limit the length of the samples. This will be useful if we plan to work with our own dataset that exceed the 1024-token context size supported by the GPT-2 model we are using for instruction fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a44a1e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "        batch,\n",
    "        pad_token_id=50256,\n",
    "        ignore_index=-100,\n",
    "        allowed_max_length=None,\n",
    "        device='cpu'\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    # Pad the prepare inputs and targets\n",
    "    inputs_list, targets_list = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "\n",
    "        # Add an `<|endoftext|>` token at the end of each sequence\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to `batch_max_length`\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "\n",
    "        # Truncate the last token for inputs\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        # Shift one position to the right for targets\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        # NEW: Replace all but the first padding tokens in targets by `ignore_index`\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # NEW: Optionally truncate sequences to `allowed_max_length`\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_list.append(inputs)\n",
    "        targets_list.append(targets)\n",
    "\n",
    "    # Convert list of inputs/targets to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_list).to(device)\n",
    "    targets_tensor = torch.stack(targets_list).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9ef9c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collated inputs:\n",
      " tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "Collated inputs shape: torch.Size([3, 5])\n",
      "Collated targets:\n",
      " tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n",
      "Collated targets shape: torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# Test the same batch with the new collate function\n",
    "collated_inputs, collated_targets = custom_collate_fn(batch)\n",
    "print(\"Collated inputs:\\n\", collated_inputs)\n",
    "print(\"Collated inputs shape:\", collated_inputs.shape)\n",
    "print(\"Collated targets:\\n\", collated_targets)\n",
    "print(\"Collated targets shape:\", collated_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50330505",
   "metadata": {},
   "source": [
    "Consider the following exampple where each output logit corresponds to a potential token from the model's vocabulary. We can calculate the cross entropy loss during training when the model predicts a sequence of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22535e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1: 1.1269280910491943\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor(\n",
    "    [[-1., 1.],   # 1st training example\n",
    "     [-0.5, 1.5]] # 2nd training example\n",
    ")\n",
    "targets_1 = torch.tensor([0, 1])\n",
    "\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(\"Loss 1:\", loss_1.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7d3b4",
   "metadata": {},
   "source": [
    "If we add an additional token ID, it would affect the loss calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c4f1e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 2: 0.7935947775840759\n"
     ]
    }
   ],
   "source": [
    "logits_2 = torch.tensor(\n",
    "    [[-1., 1.],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]] # New 3rd training example\n",
    ")\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(\"Loss 2:\", loss_2.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed11815e",
   "metadata": {},
   "source": [
    "If we set the additional token ID to `-100`, it will be ignored in the loss calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebf02a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 3: 1.1269280910491943\n"
     ]
    }
   ],
   "source": [
    "# logits_3 starts the same\n",
    "logits_3 = torch.tensor(\n",
    "    [[-1., 1.],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]]\n",
    ")\n",
    "\n",
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_3, targets_3)\n",
    "print(\"Loss 3:\", loss_3.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c46b72",
   "metadata": {},
   "source": [
    "The resulting loss only considers the valid token predictions, effectively ignoring the padding token.\n",
    "\n",
    "By default, PyTorch has the `cross_entropy` setting `ignore_index=-100`, which means that any target token with the value `-100` will be excluded from the loss computation. Using `-100` `ignore_index`, we can ignore the additional end-of-text tokens in the batches that we used to pad the training examples to equal lengths.\n",
    "\n",
    "However, we do not want to ignore the first instance of the end-of-text token because it can help signal to the LLM when the response is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a7e94",
   "metadata": {},
   "source": [
    "## 7.4 Creating Dataloaders for an Instruction Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d690a0f",
   "metadata": {},
   "source": [
    "The `custom_collate_fn` function includes a `device` parameter that moves the input and target tensors to a specified device, such as a GPU, for efficient training.\n",
    "\n",
    "Previously we moved the data onto the target device in the main training loopp. Having this as part of the collate function allows us to transfer the data as a background processs outside the training loop, preventing it from blocking the GPU during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98e3f20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    major, minor = map(int, torch.__version__.split(\".\")[:2])\n",
    "    if (major, minor) >= (2, 9):\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3e659e",
   "metadata": {},
   "source": [
    "To reuse the chosen device setting in `custom_collate_fn`, we can use the `partial` function from the `functools` module to create a new version of the collate function with the device parameter set to the desired device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1424aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "custom_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b91caf",
   "metadata": {},
   "source": [
    "Next, we set up the dataloader as we did previously, but this time we will use our customized collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb9807d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abc42183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Inputs shape: torch.Size([8, 59])\n",
      "Targets shape: torch.Size([8, 59])\n",
      "Inputs shape: torch.Size([8, 65])\n",
      "Targets shape: torch.Size([8, 65])\n",
      "Inputs shape: torch.Size([8, 69])\n",
      "Targets shape: torch.Size([8, 69])\n",
      "Inputs shape: torch.Size([8, 73])\n",
      "Targets shape: torch.Size([8, 73])\n",
      "Inputs shape: torch.Size([8, 62])\n",
      "Targets shape: torch.Size([8, 62])\n",
      "Inputs shape: torch.Size([8, 60])\n",
      "Targets shape: torch.Size([8, 60])\n",
      "Inputs shape: torch.Size([8, 71])\n",
      "Targets shape: torch.Size([8, 71])\n",
      "Inputs shape: torch.Size([8, 60])\n",
      "Targets shape: torch.Size([8, 60])\n",
      "Inputs shape: torch.Size([8, 80])\n",
      "Targets shape: torch.Size([8, 80])\n",
      "Inputs shape: torch.Size([8, 76])\n",
      "Targets shape: torch.Size([8, 76])\n",
      "Inputs shape: torch.Size([8, 74])\n",
      "Targets shape: torch.Size([8, 74])\n",
      "Inputs shape: torch.Size([8, 62])\n",
      "Targets shape: torch.Size([8, 62])\n",
      "Inputs shape: torch.Size([8, 68])\n",
      "Targets shape: torch.Size([8, 68])\n",
      "Inputs shape: torch.Size([8, 58])\n",
      "Targets shape: torch.Size([8, 58])\n",
      "Inputs shape: torch.Size([8, 83])\n",
      "Targets shape: torch.Size([8, 83])\n",
      "Inputs shape: torch.Size([8, 91])\n",
      "Targets shape: torch.Size([8, 91])\n",
      "Inputs shape: torch.Size([8, 65])\n",
      "Targets shape: torch.Size([8, 65])\n",
      "Inputs shape: torch.Size([8, 71])\n",
      "Targets shape: torch.Size([8, 71])\n",
      "Inputs shape: torch.Size([8, 67])\n",
      "Targets shape: torch.Size([8, 67])\n",
      "Inputs shape: torch.Size([8, 77])\n",
      "Targets shape: torch.Size([8, 77])\n",
      "Inputs shape: torch.Size([8, 79])\n",
      "Targets shape: torch.Size([8, 79])\n",
      "Inputs shape: torch.Size([8, 71])\n",
      "Targets shape: torch.Size([8, 71])\n",
      "Inputs shape: torch.Size([8, 68])\n",
      "Targets shape: torch.Size([8, 68])\n",
      "Inputs shape: torch.Size([8, 70])\n",
      "Targets shape: torch.Size([8, 70])\n",
      "Inputs shape: torch.Size([8, 75])\n",
      "Targets shape: torch.Size([8, 75])\n",
      "Inputs shape: torch.Size([8, 74])\n",
      "Targets shape: torch.Size([8, 74])\n",
      "Inputs shape: torch.Size([8, 82])\n",
      "Targets shape: torch.Size([8, 82])\n",
      "Inputs shape: torch.Size([8, 68])\n",
      "Targets shape: torch.Size([8, 68])\n",
      "Inputs shape: torch.Size([8, 59])\n",
      "Targets shape: torch.Size([8, 59])\n",
      "Inputs shape: torch.Size([8, 67])\n",
      "Targets shape: torch.Size([8, 67])\n",
      "Inputs shape: torch.Size([8, 67])\n",
      "Targets shape: torch.Size([8, 67])\n",
      "Inputs shape: torch.Size([8, 78])\n",
      "Targets shape: torch.Size([8, 78])\n",
      "Inputs shape: torch.Size([8, 77])\n",
      "Targets shape: torch.Size([8, 77])\n",
      "Inputs shape: torch.Size([8, 68])\n",
      "Targets shape: torch.Size([8, 68])\n",
      "Inputs shape: torch.Size([8, 89])\n",
      "Targets shape: torch.Size([8, 89])\n",
      "Inputs shape: torch.Size([8, 62])\n",
      "Targets shape: torch.Size([8, 62])\n",
      "Inputs shape: torch.Size([8, 63])\n",
      "Targets shape: torch.Size([8, 63])\n",
      "Inputs shape: torch.Size([8, 63])\n",
      "Targets shape: torch.Size([8, 63])\n",
      "Inputs shape: torch.Size([8, 69])\n",
      "Targets shape: torch.Size([8, 69])\n",
      "Inputs shape: torch.Size([8, 88])\n",
      "Targets shape: torch.Size([8, 88])\n",
      "Inputs shape: torch.Size([8, 64])\n",
      "Targets shape: torch.Size([8, 64])\n",
      "Inputs shape: torch.Size([8, 65])\n",
      "Targets shape: torch.Size([8, 65])\n",
      "Inputs shape: torch.Size([8, 67])\n",
      "Targets shape: torch.Size([8, 67])\n",
      "Inputs shape: torch.Size([8, 65])\n",
      "Targets shape: torch.Size([8, 65])\n",
      "Inputs shape: torch.Size([8, 75])\n",
      "Targets shape: torch.Size([8, 75])\n",
      "Inputs shape: torch.Size([8, 59])\n",
      "Targets shape: torch.Size([8, 59])\n",
      "Inputs shape: torch.Size([8, 61])\n",
      "Targets shape: torch.Size([8, 61])\n",
      "Inputs shape: torch.Size([8, 76])\n",
      "Targets shape: torch.Size([8, 76])\n",
      "Inputs shape: torch.Size([8, 74])\n",
      "Targets shape: torch.Size([8, 74])\n",
      "Inputs shape: torch.Size([8, 64])\n",
      "Targets shape: torch.Size([8, 64])\n",
      "Inputs shape: torch.Size([8, 68])\n",
      "Targets shape: torch.Size([8, 68])\n",
      "Inputs shape: torch.Size([8, 83])\n",
      "Targets shape: torch.Size([8, 83])\n",
      "Inputs shape: torch.Size([8, 58])\n",
      "Targets shape: torch.Size([8, 58])\n",
      "Inputs shape: torch.Size([8, 57])\n",
      "Targets shape: torch.Size([8, 57])\n",
      "Inputs shape: torch.Size([8, 61])\n",
      "Targets shape: torch.Size([8, 61])\n",
      "Inputs shape: torch.Size([8, 77])\n",
      "Targets shape: torch.Size([8, 77])\n",
      "Inputs shape: torch.Size([8, 63])\n",
      "Targets shape: torch.Size([8, 63])\n",
      "Inputs shape: torch.Size([8, 67])\n",
      "Targets shape: torch.Size([8, 67])\n",
      "Inputs shape: torch.Size([8, 72])\n",
      "Targets shape: torch.Size([8, 72])\n",
      "Inputs shape: torch.Size([8, 65])\n",
      "Targets shape: torch.Size([8, 65])\n",
      "Inputs shape: torch.Size([8, 64])\n",
      "Targets shape: torch.Size([8, 64])\n",
      "Inputs shape: torch.Size([8, 66])\n",
      "Targets shape: torch.Size([8, 66])\n",
      "Inputs shape: torch.Size([8, 68])\n",
      "Targets shape: torch.Size([8, 68])\n",
      "Inputs shape: torch.Size([8, 64])\n",
      "Targets shape: torch.Size([8, 64])\n",
      "Inputs shape: torch.Size([8, 67])\n",
      "Targets shape: torch.Size([8, 67])\n",
      "Inputs shape: torch.Size([8, 69])\n",
      "Targets shape: torch.Size([8, 69])\n",
      "Inputs shape: torch.Size([8, 59])\n",
      "Targets shape: torch.Size([8, 59])\n",
      "Inputs shape: torch.Size([8, 71])\n",
      "Targets shape: torch.Size([8, 71])\n",
      "Inputs shape: torch.Size([8, 75])\n",
      "Targets shape: torch.Size([8, 75])\n",
      "Inputs shape: torch.Size([8, 76])\n",
      "Targets shape: torch.Size([8, 76])\n",
      "Inputs shape: torch.Size([8, 74])\n",
      "Targets shape: torch.Size([8, 74])\n",
      "Inputs shape: torch.Size([8, 65])\n",
      "Targets shape: torch.Size([8, 65])\n",
      "Inputs shape: torch.Size([8, 83])\n",
      "Targets shape: torch.Size([8, 83])\n",
      "Inputs shape: torch.Size([8, 70])\n",
      "Targets shape: torch.Size([8, 70])\n",
      "Inputs shape: torch.Size([8, 72])\n",
      "Targets shape: torch.Size([8, 72])\n",
      "Inputs shape: torch.Size([8, 81])\n",
      "Targets shape: torch.Size([8, 81])\n",
      "Inputs shape: torch.Size([8, 91])\n",
      "Targets shape: torch.Size([8, 91])\n",
      "Inputs shape: torch.Size([8, 83])\n",
      "Targets shape: torch.Size([8, 83])\n",
      "Inputs shape: torch.Size([8, 65])\n",
      "Targets shape: torch.Size([8, 65])\n",
      "Inputs shape: torch.Size([8, 68])\n",
      "Targets shape: torch.Size([8, 68])\n",
      "Inputs shape: torch.Size([8, 83])\n",
      "Targets shape: torch.Size([8, 83])\n",
      "Inputs shape: torch.Size([8, 63])\n",
      "Targets shape: torch.Size([8, 63])\n",
      "Inputs shape: torch.Size([8, 70])\n",
      "Targets shape: torch.Size([8, 70])\n",
      "Inputs shape: torch.Size([8, 60])\n",
      "Targets shape: torch.Size([8, 60])\n",
      "Inputs shape: torch.Size([8, 74])\n",
      "Targets shape: torch.Size([8, 74])\n",
      "Inputs shape: torch.Size([8, 73])\n",
      "Targets shape: torch.Size([8, 73])\n",
      "Inputs shape: torch.Size([8, 71])\n",
      "Targets shape: torch.Size([8, 71])\n",
      "Inputs shape: torch.Size([8, 83])\n",
      "Targets shape: torch.Size([8, 83])\n",
      "Inputs shape: torch.Size([8, 63])\n",
      "Targets shape: torch.Size([8, 63])\n",
      "Inputs shape: torch.Size([8, 69])\n",
      "Targets shape: torch.Size([8, 69])\n",
      "Inputs shape: torch.Size([8, 64])\n",
      "Targets shape: torch.Size([8, 64])\n",
      "Inputs shape: torch.Size([8, 66])\n",
      "Targets shape: torch.Size([8, 66])\n",
      "Inputs shape: torch.Size([8, 71])\n",
      "Targets shape: torch.Size([8, 71])\n",
      "Inputs shape: torch.Size([8, 80])\n",
      "Targets shape: torch.Size([8, 80])\n",
      "Inputs shape: torch.Size([8, 64])\n",
      "Targets shape: torch.Size([8, 64])\n",
      "Inputs shape: torch.Size([8, 71])\n",
      "Targets shape: torch.Size([8, 71])\n",
      "Inputs shape: torch.Size([8, 68])\n",
      "Targets shape: torch.Size([8, 68])\n",
      "Inputs shape: torch.Size([8, 72])\n",
      "Targets shape: torch.Size([8, 72])\n",
      "Inputs shape: torch.Size([8, 73])\n",
      "Targets shape: torch.Size([8, 73])\n",
      "Inputs shape: torch.Size([8, 65])\n",
      "Targets shape: torch.Size([8, 65])\n",
      "Inputs shape: torch.Size([8, 75])\n",
      "Targets shape: torch.Size([8, 75])\n",
      "Inputs shape: torch.Size([8, 80])\n",
      "Targets shape: torch.Size([8, 80])\n",
      "Inputs shape: torch.Size([8, 66])\n",
      "Targets shape: torch.Size([8, 66])\n",
      "Inputs shape: torch.Size([8, 70])\n",
      "Targets shape: torch.Size([8, 70])\n",
      "Inputs shape: torch.Size([8, 68])\n",
      "Targets shape: torch.Size([8, 68])\n",
      "Inputs shape: torch.Size([8, 67])\n",
      "Targets shape: torch.Size([8, 67])\n",
      "Inputs shape: torch.Size([8, 91])\n",
      "Targets shape: torch.Size([8, 91])\n",
      "Inputs shape: torch.Size([8, 72])\n",
      "Targets shape: torch.Size([8, 72])\n",
      "Inputs shape: torch.Size([8, 56])\n",
      "Targets shape: torch.Size([8, 56])\n",
      "Inputs shape: torch.Size([8, 70])\n",
      "Targets shape: torch.Size([8, 70])\n",
      "Inputs shape: torch.Size([8, 87])\n",
      "Targets shape: torch.Size([8, 87])\n",
      "Inputs shape: torch.Size([8, 69])\n",
      "Targets shape: torch.Size([8, 69])\n",
      "Inputs shape: torch.Size([8, 68])\n",
      "Targets shape: torch.Size([8, 68])\n",
      "Inputs shape: torch.Size([8, 67])\n",
      "Targets shape: torch.Size([8, 67])\n",
      "Inputs shape: torch.Size([8, 67])\n",
      "Targets shape: torch.Size([8, 67])\n",
      "Inputs shape: torch.Size([8, 66])\n",
      "Targets shape: torch.Size([8, 66])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(\"Inputs shape:\", inputs.shape)\n",
    "    print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27402190",
   "metadata": {},
   "source": [
    "All batches have a batch size of 8 but a different sequence length depending on the longest sequence in each batch as expected.\n",
    "\n",
    "We also need to check the `<|endoftext|>` token in the target batches to ensure that only one instance of this token is present in each target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56ea5570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example input sequence:\n",
      "tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
      "        21017, 46486,    25,   198, 15946,   485,   257,  1573,   326,  9529,\n",
      "        22009,   351,   366,  6651,   526,   198,   198, 21017, 18261,    25,\n",
      "          198,    32,  1573,   326,  9529, 22009,   351,   366,  6651,     1,\n",
      "          318,   366,  9496,   526, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256])\n",
      "Example target sequence:\n",
      "tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,\n",
      "         2882,   326, 20431, 32543,   262,  2581,    13,   198,   198, 21017,\n",
      "        46486,    25,   198, 15946,   485,   257,  1573,   326,  9529, 22009,\n",
      "          351,   366,  6651,   526,   198,   198, 21017, 18261,    25,   198,\n",
      "           32,  1573,   326,  9529, 22009,   351,   366,  6651,     1,   318,\n",
      "          366,  9496,   526, 50256,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "print(\"Example input sequence:\")\n",
    "print(inputs[0])\n",
    "\n",
    "print(\"Example target sequence:\")\n",
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a849d42c",
   "metadata": {},
   "source": [
    "## 7.5 Loading a Pretrained LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4868b4e",
   "metadata": {},
   "source": [
    "We need to load the medium-sized GPT-2 model with 355M parameters as our base LLM for instruction fine-tuning because the small GPT-2 model with 124M parameters may not have sufficient capacity to effectively learn and generalize from the instruction dataset. The storage of the medium-sized model is approximately 1.5GB, which is about three times larger than the small model's size of approximately 500MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3d4558d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Repos\\build_a_large_language_model_from_scratch\\.venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n",
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 117kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.85MiB/s]\n",
      "hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 92.4kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G [01:47<00:00, 13.2MiB/s]\n",
      "model.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 12.6MiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 927k/927k [00:00<00:00, 2.63MiB/s]\n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 1.76MiB/s]\n"
     ]
    }
   ],
   "source": [
    "from llms_from_scratch.ch04 import GPTModel\n",
    "from llms_from_scratch.ch05 import download_and_load_gpt2, load_weights_into_gpt\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0777b3",
   "metadata": {},
   "source": [
    "We need to test the performance of the medium-sized GPT-2 model on our instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a13d1ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input text:\n",
      " Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Evaluate the following phrase by transforming it into the spelling given.\n",
      "\n",
      "### Input:\n",
      "freind --> friend\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "input_text = format_input(data[0])\n",
    "print(\"Sample input text:\\n\", input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2857f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms_from_scratch.ch05 import generate, text_to_token_ids, token_ids_to_text\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=100,\n",
    "    context_size=BASE_CONFIG['context_length'],\n",
    "    eos_id=50256\n",
    ")\n",
    "\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ea60c",
   "metadata": {},
   "source": [
    "The `generate` function returns the combined input and output text, which is convenient in the previous section for creating legible text.\n",
    "\n",
    "Now we need to separate the generated output text from the input prompt to evaluate the model's responses accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2eb7609c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response:\n",
      " ### Output:\n",
      "\n",
      "freind --> friend\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Evaluate the following phrase by transforming it into the spelling given.\n",
      "\n",
      "### Input:\n",
      "\n",
      "freind --> friend\n",
      "\n",
      "### Output:\n",
      "\n",
      "freind --> friend\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Evaluate the following phrase by transforming it into the spelling given.\n",
      "\n",
      "### Input:\n",
      "\n",
      "freind --> friend\n",
      "\n",
      "### Output:\n",
      "\n",
      "freind --> friend\n",
      "\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "print(\"Generated response:\\n\", response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae516c0",
   "metadata": {},
   "source": [
    "## 7.6 Fine-Tuning the LLM on Instruction Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092be97",
   "metadata": {},
   "source": [
    "For the fine-tuning process, we will reuse the loss calculation and training functions implemented in Chapter 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ce3acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms_from_scratch.ch05 import calc_loss_loader, train_model_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44271878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial train loss: 3.8946\n",
      "Initial val loss: 3.7619\n"
     ]
    }
   ],
   "source": [
    "# Calculate initial losses\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "print(f\"Initial train loss: {train_loss:.4f}\")\n",
    "print(f\"Initial val loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "638e9503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.583, Val loss 2.556\n",
      "Ep 1 (Step 000005): Train loss 0.965, Val loss 1.093\n",
      "Ep 1 (Step 000010): Train loss 0.857, Val loss 0.959\n",
      "Ep 1 (Step 000015): Train loss 0.831, Val loss 0.914\n",
      "Ep 1 (Step 000020): Train loss 0.728, Val loss 0.883\n",
      "Ep 1 (Step 000025): Train loss 0.776, Val loss 0.854\n",
      "Ep 1 (Step 000030): Train loss 0.783, Val loss 0.839\n",
      "Ep 1 (Step 000035): Train loss 0.707, Val loss 0.808\n",
      "Ep 1 (Step 000040): Train loss 0.715, Val loss 0.790\n",
      "Ep 1 (Step 000045): Train loss 0.751, Val loss 0.777\n",
      "Ep 1 (Step 000050): Train loss 0.710, Val loss 0.763\n",
      "Ep 1 (Step 000055): Train loss 0.646, Val loss 0.741\n",
      "Ep 1 (Step 000060): Train loss 0.566, Val loss 0.727\n",
      "Ep 1 (Step 000065): Train loss 0.576, Val loss 0.726\n",
      "Ep 1 (Step 000070): Train loss 0.535, Val loss 0.718\n",
      "Ep 1 (Step 000075): Train loss 0.595, Val loss 0.715\n",
      "Ep 1 (Step 000080): Train loss 0.544, Val loss 0.711\n",
      "Ep 1 (Step 000085): Train loss 0.495, Val loss 0.697\n",
      "Ep 1 (Step 000090): Train loss 0.486, Val loss 0.689\n",
      "Ep 1 (Step 000095): Train loss 0.515, Val loss 0.683\n",
      "Ep 1 (Step 000100): Train loss 0.575, Val loss 0.673\n",
      "Ep 1 (Step 000105): Train loss 0.443, Val loss 0.670\n",
      "Ep 1 (Step 000110): Train loss 0.427, Val loss 0.673\n",
      "Ep 1 (Step 000115): Train loss 0.503, Val loss 0.669\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom\n",
      "Ep 2 (Step 000120): Train loss 0.483, Val loss 0.663\n",
      "Ep 2 (Step 000125): Train loss 0.388, Val loss 0.660\n",
      "Ep 2 (Step 000130): Train loss 0.449, Val loss 0.672\n",
      "Ep 2 (Step 000135): Train loss 0.446, Val loss 0.675\n",
      "Ep 2 (Step 000140): Train loss 0.388, Val loss 0.670\n",
      "Ep 2 (Step 000145): Train loss 0.424, Val loss 0.667\n",
      "Ep 2 (Step 000150): Train loss 0.356, Val loss 0.662\n",
      "Ep 2 (Step 000155): Train loss 0.416, Val loss 0.660\n",
      "Ep 2 (Step 000160): Train loss 0.402, Val loss 0.666\n",
      "Ep 2 (Step 000165): Train loss 0.419, Val loss 0.664\n",
      "Ep 2 (Step 000170): Train loss 0.373, Val loss 0.655\n",
      "Ep 2 (Step 000175): Train loss 0.372, Val loss 0.653\n",
      "Ep 2 (Step 000180): Train loss 0.403, Val loss 0.657\n",
      "Ep 2 (Step 000185): Train loss 0.318, Val loss 0.662\n",
      "Ep 2 (Step 000190): Train loss 0.337, Val loss 0.673\n",
      "Ep 2 (Step 000195): Train loss 0.379, Val loss 0.655\n",
      "Ep 2 (Step 000200): Train loss 0.367, Val loss 0.643\n",
      "Ep 2 (Step 000205): Train loss 0.328, Val loss 0.653\n",
      "Ep 2 (Step 000210): Train loss 0.356, Val loss 0.647\n",
      "Ep 2 (Step 000215): Train loss 0.336, Val loss 0.652\n",
      "Ep 2 (Step 000220): Train loss 0.291, Val loss 0.652\n",
      "Ep 2 (Step 000225): Train loss 0.337, Val loss 0.663\n",
      "Ep 2 (Step 000230): Train loss 0.345, Val loss 0.659\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive:\n",
      "Training completed in 9.45 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=format_input(val_data[0]),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93602589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llms_from_scratch.ch05 import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811b808c",
   "metadata": {},
   "source": [
    "## 7.7 Extracting and Saving Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6aff1",
   "metadata": {},
   "source": [
    "Now we are ready to evaluate the fine-tuned LLM on the instruction dataset by extracting the model-generated responses for each input in the test dataset and collecting them for manual analysis, and then evaluating the LLM to quantify the quality of the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95dbce08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> A thunderstorm is a type of cloud that typically forms in the atmosphere at altitudes of at least 10 kilometers.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG['context_length'],\n",
    "        eos_id=50256\n",
    "    )\n",
    "\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99343bf0",
   "metadata": {},
   "source": [
    "Model evaluation for instruction fine-tuning is not as straightforward as the perplexity metric used for LLM pretraining or the classification accuracy metric used for discriminative tasks.\n",
    "\n",
    "In practice, instruction-fine-tuned LLMs such as chatbots are evaluted via multiple approaches:\n",
    "- Short-answer and multiple-choice benchmarks, such as *Measuring Massive Multitask Language Understanding* (MMLU), which test the general knowledge of a model.\n",
    "- Human preference comparison to other LLMs, such as LMSYS chatbot arena.\n",
    "- Automated conversational benchmarks, where another LLM like GPT-4 is used to evaluate the responses, such as AlpacaEval.\n",
    "\n",
    "In practice, it can be helpful to consider all three types of evaluation methods: multiple-choice question answering, human evaluation, and automated metrics that measure conversational performance.\n",
    "\n",
    "For demo purposes, we will implement an approach similar to automated conversational benchmarks, which involves evaluating the responses automatically using another LLM.\n",
    "\n",
    "We will use our own test dataset for evaluation, so we need to prepare the responses for this evalution process. We will append the generated model responses to the `test_set` dictionary and save the updated data as an `instruction-data-with-response.json` file for record keeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "544c2f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [04:48<00:00,  2.62s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG['context_length'],\n",
    "        eos_id=50256\n",
    "    )\n",
    "\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "    test_data[i]['model_response'] = response_text\n",
    "\n",
    "with open('instruction-data-with-responses.json', 'w') as f:\n",
    "    json.dump(test_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62f46edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_response': 'The car is as fast as a bullet.'}\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e584782",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeeab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa448fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model via\n",
    "model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb08b029",
   "metadata": {},
   "source": [
    "## 7.8 Evaluating the Fine-Tuned LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ab8321",
   "metadata": {},
   "source": [
    "To evaluate test set responses in an automated manner, we will use an instruction-fine-tuned 8M parameter Llama-3 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7ae911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb2f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is optional; it allows you to restart the notebook\n",
    "# and only run section 7.7 without rerunning any of the previous code\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = \"instruction-data-with-response.json\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ac060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # noqa: F811\n",
    "# import urllib.request\n",
    "\n",
    "def query_model(\n",
    "    prompt,\n",
    "    model=\"llama3\",\n",
    "    # If you used OLLAMA_HOST=127.0.0.1:11435 ollama serve\n",
    "    # update the address from 11434 to 11435\n",
    "    url=\"http://localhost:11434/api/chat\"\n",
    "):\n",
    "    # Create the data payload as a dictionary\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": {     # Settings below are required for deterministic responses\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    # Convert the dictionary to a JSON formatted string and encode it to bytes\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "\n",
    "    # Create a request object, setting the method to POST and adding necessary headers\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=payload,\n",
    "        method=\"POST\"\n",
    "    )\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # Send the request and capture the response\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        # Read and decode the response\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data\n",
    "    \"\"\"\n",
    "\n",
    "    # The book originally used the commented-out above, which is based\n",
    "    # on urllib. It works generally fine, but some readers reported\n",
    "    # issues with using urlib when using a (company) VPN.\n",
    "    # The code below uses the requests library, which doesn't seem\n",
    "    # to have these issues.\n",
    "\n",
    "    # Send the POST request\n",
    "    with requests.post(url, json=data, stream=True, timeout=30) as r:\n",
    "        r.raise_for_status()\n",
    "        response_data = \"\"\n",
    "        for line in r.iter_lines(decode_unicode=True):\n",
    "            if not line:\n",
    "                continue\n",
    "            response_json = json.loads(line)\n",
    "            if \"message\" in response_json:\n",
    "                response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data\n",
    "\n",
    "\n",
    "model = \"llama3\"\n",
    "result = query_model(\"What do Llamas eat?\", model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55af050",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in test_data[:3]:\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"score the model response `{entry['model_response']}`\"\n",
    "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model_response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a0c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_scores(json_data, json_key, model=\"llama3\"):\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2518ddc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
