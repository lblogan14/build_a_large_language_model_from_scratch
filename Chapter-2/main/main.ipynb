{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "774dfe83",
   "metadata": {},
   "source": [
    "# Chapter 2: Working with Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db12b85",
   "metadata": {},
   "source": [
    "## 2.2 Tokenizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "900da2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1\n",
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"PyTorch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d70b9c3",
   "metadata": {},
   "source": [
    "In this section, we wil tokenize text into smaller units, such as individual words and punctuation characters.\n",
    "\n",
    "Before that, we will load raw text we want to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e02b14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "    file_path = \"the-verdict.txt\"\n",
    "\n",
    "    response = requests.get(url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd64fc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "First 100 characters:\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(f\"Total number of characters: {len(raw_text)}\")\n",
    "print(f\"First 100 characters:\\n{raw_text[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0e5819f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world!', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "# To start with, we will use `re` to tokenize the text into words and punctuation.\n",
    "import re\n",
    "\n",
    "text = \"Hello, world! This is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae289e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world!', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba80b19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world!', 'This', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "# Strip whitespace from each item and then filter out any empty strings.\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfabf146",
   "metadata": {},
   "source": [
    "**NOTE:** When developing a simple tokenizer, whether we should encode whitespaces as separate characters or ignore them depends on our application and its requirements. Removing whitespaces reduces the memory and computing power, but keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example, Python code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56322fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "# Final tokenizer implementation\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bedfd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer on the raw text\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "# Print the first 30 tokens\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f0eb9b",
   "metadata": {},
   "source": [
    "## 2.3 Converting Tokens into Token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1cf0c6",
   "metadata": {},
   "source": [
    "To convert textual tokens into numerical representations that machine learning models can process, we need to map each token to a unique integer ID. This process is essential for feeding text data into models like neural networks.\n",
    "\n",
    "Before that, we need to build a vocabulary that defines how we map each unique word and special character to an integer. This vocabulary acts as a dictionary for the model to understand the input data.\n",
    "\n",
    "From these tokens, we can build a vocabulary by assigning a unique integer ID to each unique token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4a6920b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a687c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n"
     ]
    }
   ],
   "source": [
    "# Create a vocabulary mapping from token to ID\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "# Display the first 50 items in the vocabulary\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    if i >= 50:\n",
    "        break\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277e82ea",
   "metadata": {},
   "source": [
    "Next, we will apply the vocabulary to convert text tokens into their corresponding token IDs. When we want to convert the outputs of an LLM from numbers back into text, we can use the reverse mapping from token IDs to tokens.\n",
    "\n",
    "To do this, we will implement a tokenizer class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "730d5b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Tokenize the input text\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        # Convert tokens to token IDs\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuation marks\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d58f271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f3c5b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "793f528f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b546623",
   "metadata": {},
   "source": [
    "This looks good so far, but it will occur an error if we try to encode a token that is not in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adeaa269",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      9\u001b[39m preprocessed = [item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()]\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Convert tokens to token IDs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48264f8",
   "metadata": {},
   "source": [
    "## 2.4 Adding Special Context Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54734984",
   "metadata": {},
   "source": [
    "To handle unknown words and address the usage and addition of special context tokens, we can enhance our tokenizer class.\n",
    "\n",
    "We will add two special tokens to our vocabulary:\n",
    "- `<|unk|>` for unknown words\n",
    "- `<|endoftext|>` to signify the end of a text sequence.\n",
    "\n",
    "When training GPT-like LLMs on multiple independent documents or books, it is common to insert a token before each document or book that follows a previous text source. This helps the LLM understand that although these text sources are concatenated for training purposes, they are independent of each other.\n",
    "\n",
    "Now we will modify our tokenizer class and vocabulary to include these special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc5dc3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated vocabulary size: 1132\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
    "\n",
    "# Update the vocabulary to include special tokens\n",
    "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n",
    "\n",
    "print(f\"Updated vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4913b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|unk|>', 1130)\n",
      "('<|endoftext|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "# Print the last 5 items in the updated vocabulary\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12190631",
   "metadata": {},
   "source": [
    "Next we will update our tokenizer to handle unknown tokens gracefully by mapping them to the `<unk>` token ID during encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bda7173",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Tokenize the input text\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        # Convert unknown tokens to <unk>\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        # Convert tokens to token IDs\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71ab3ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd27d597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1130, 5, 355, 1126, 628, 975, 10, 1131, 55, 988, 956, 984, 722, 988, 1130, 7]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1f99fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1759816",
   "metadata": {},
   "source": [
    "Depending on the LLM, some researchers also consider additional special tokens:\n",
    "- `[BOS]` (beginning of sequence) to mark the start of a text sequence. It signifies to the model where a piece of content begins.\n",
    "- `[EOS]` (end of sequence) to mark the end of a text sequence. Similar to `<|endoftext|>`, it indicates where a piece of content concludes.\n",
    "- `[PAD]` (padding) to fill in sequences to a uniform length when training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. The `[PAD]` token is used to extend shorter sequences to match the length of the longest sequence in the batch, ensuring that all sequences have the same length for efficient processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816f719c",
   "metadata": {},
   "source": [
    "## 2.5 Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e0208",
   "metadata": {},
   "source": [
    "The **Byte Pair Encoding (BPE)** tokenizer was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT. \n",
    "\n",
    "BPE allows the model to break down words that are not in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words more effectively.\n",
    "\n",
    "We will explore how BPE works by using an existing BPE implementation from the `tiktoken` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e55ea2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf7213bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BPE tokenizer for GPT-2\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f75c3777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "# Test the BPE tokenizer\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "\n",
    "ids = tokenizer.encode(\n",
    "    text,\n",
    "    allowed_special={\"<|endoftext|>\"}\n",
    ")\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a864220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(ids)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d596c556",
   "metadata": {},
   "source": [
    "The `<|endoftext|>` token is assigned a relatively large token ID (50256) to avoid conflicts with common tokens in the vocabulary.\n",
    "\n",
    "The BPE tokenizer can also handle unknown words, such as `\"someunknownPlace\"`, correctly breaking it down into smaller subword units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcde361b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11246, 34680, 27271]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"someunknownPlace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa9fdce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11246, 555, 74, 8474]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"some unknown Place\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb47df",
   "metadata": {},
   "source": [
    "## 2.6 Data Sampling with a Sliding Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f024cfd4",
   "metadata": {},
   "source": [
    "Our next step is to generate the input-target pairs required for training an LLM. LLMs are pretrained by predicting the one word at a time, so we need to prepare the training data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6652001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "# Load the text data\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# Encode the entire text using the BPE tokenizer\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(f\"Total number of tokens: {len(enc_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ade38",
   "metadata": {},
   "source": [
    "For demo purposes, we will remove the first 50 tokens from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a77e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_smaple = enc_text[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c11144c",
   "metadata": {},
   "source": [
    "Since we want the model to predict the next word, the targets are the inputs shifted by one position to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70e4ec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_smaple[:context_size]\n",
    "y = enc_smaple[1 : context_size + 1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8640eeda",
   "metadata": {},
   "source": [
    "The *context size* defines how many tokens are included in the input.\n",
    "\n",
    "Now we can create input-target pairs using a sliding window approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "334f0b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [290] -> Desired: 4920\n",
      "Context: [290, 4920] -> Desired: 2241\n",
      "Context: [290, 4920, 2241] -> Desired: 287\n",
      "Context: [290, 4920, 2241, 287] -> Desired: 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_smaple[:i]\n",
    "    desired = enc_smaple[i]\n",
    "\n",
    "    print(f\"Context: {context} -> Desired: {desired}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c49e5260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ' and' -> Desired: ' established'\n",
      "Context: ' and established' -> Desired: ' himself'\n",
      "Context: ' and established himself' -> Desired: ' in'\n",
      "Context: ' and established himself in' -> Desired: ' a'\n"
     ]
    }
   ],
   "source": [
    "# Convert token IDs back to strings for better readability\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_smaple[:i]\n",
    "    desired = enc_smaple[i]\n",
    "\n",
    "    context_str = tokenizer.decode(context)\n",
    "    desired_str = tokenizer.decode([desired])\n",
    "\n",
    "    print(f\"Context: '{context_str}' -> Desired: '{desired_str}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512f7ce2",
   "metadata": {},
   "source": [
    "Next, we will implement an efficient dataloader that iterates over the input dataset and returns the inputs and targets as PyTorch tensors for training. There are two tensors: an input tensor containing the text that the LLM sees and a target tensor containing the expected outputs (the next token for each position in the input).\n",
    "\n",
    "We will use `Dataset` and `DataLoader` classes from the `torch.utils.data` module to create a custom dataset and dataloader for our tokenized text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c3cbd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a28918e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(\n",
    "            txt,\n",
    "            allowed_special={\"<|endoftext|>\"}\n",
    "        )\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to `max_length`.\"\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of `max_length` tokens\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i+1 : i + max_length+1]\n",
    "\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3a060e",
   "metadata": {},
   "source": [
    "Then we use the `GPTDatasetV1` class to load the inputs in batches via a `DataLoader` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfd84b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(\n",
    "    txt,\n",
    "    batch_size=4,\n",
    "    max_length=256,\n",
    "    stride=128,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0\n",
    "):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = GPTDatasetV1(\n",
    "        txt,\n",
    "        tokenizer,\n",
    "        max_length,\n",
    "        stride\n",
    "    )\n",
    "\n",
    "    # Create the dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d40c0d4",
   "metadata": {},
   "source": [
    "Test the `dataloader` with a batch size of 1 for an LLM with a context size of 4 to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b480602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[  40,  367, 2885, 1464]])\n",
      "Target IDs: tensor([[ 367, 2885, 1464, 1807]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,\n",
    "    batch_size=1,\n",
    "    max_length=4,\n",
    "    stride=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(\"Input IDs:\", first_batch[0])\n",
    "print(\"Target IDs:\", first_batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ac1a9d",
   "metadata": {},
   "source": [
    "The `first_batch` variable contains two tensors: the input IDs and the target IDs. Since the `max_length` is set to 4, each tensor has a shape of `(1, 4)`, indicating one sequence of four tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dcffece8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 367, 2885, 1464, 1807]])\n",
      "Target IDs: tensor([[2885, 1464, 1807, 3619]])\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(\"Input IDs:\", second_batch[0])\n",
    "print(\"Target IDs:\", second_batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6122008e",
   "metadata": {},
   "source": [
    "Comparing the first and second batches, we can see that the second batch's token IDs are shifted by **one position** compared to the first batch. The `stride` parameter in the `GPTDatasetV1` class controls this shifting behavior.\n",
    "\n",
    "If we use the dataloader to sample with a larger batch size and different stride:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c114170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Target IDs:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,\n",
    "    batch_size=8,\n",
    "    max_length=4,\n",
    "    stride=4,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Input IDs:\\n\", inputs)\n",
    "print(\"Target IDs:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8520937b",
   "metadata": {},
   "source": [
    "The stride of 4 avoids any overlap between the batches since more overlap could lead to increased overfitting during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10734771",
   "metadata": {},
   "source": [
    "## 2.7 Creating Token Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34314be3",
   "metadata": {},
   "source": [
    "The data is almost ready for training an LLM, but lastly we need to embed the tokens in a continuous vector representation using an embedding layer. Usually these embedding layers are part of the LLM architecture itself and are updated (trained) during model training.\n",
    "\n",
    "Suppose we have the following four input token IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4f82a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b78d21",
   "metadata": {},
   "source": [
    "For demo purpose, suppose we have a small vocabulary of only 6 words and an embedding dimension of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d42025cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.1258, -1.1524,  0.5667],\n",
      "        [ 0.7935,  0.5988, -1.5551],\n",
      "        [-0.3414,  1.8530,  0.4681],\n",
      "        [-0.1577, -0.1734,  0.1835],\n",
      "        [ 1.3894,  1.5863,  0.9463],\n",
      "        [-0.8437,  0.9318,  1.2590]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(0)\n",
    "embedding_layer = torch.nn.Embedding(\n",
    "    num_embeddings=vocab_size,\n",
    "    embedding_dim=output_dim\n",
    ")\n",
    "\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000921ec",
   "metadata": {},
   "source": [
    "This weight matrix has 6 rows (one for each token in the vocabulary) and 3 columns (the embedding dimension). \n",
    "\n",
    "Now we can apply it to a token ID to obtain the corresponding embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ca2fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1577, -0.1734,  0.1835]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0df3d6",
   "metadata": {},
   "source": [
    "This will output the embedding vector for the token ID `3` from the embedding layer. This is the 4th row of the embedding weight matrix, corresponding to the token with ID `3`.\n",
    "\n",
    "Now we can use this embedding layer to convert all input token IDs into their corresponding embedding vectors, which can then be fed into the LLM for training or inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "392299d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3414,  1.8530,  0.4681],\n",
      "        [-0.1577, -0.1734,  0.1835],\n",
      "        [-0.8437,  0.9318,  1.2590],\n",
      "        [ 0.7935,  0.5988, -1.5551]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aada6bc",
   "metadata": {},
   "source": [
    "## 2.8 Encoding Word Positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9975187",
   "metadata": {},
   "source": [
    "The self-attention mechanism in LLMs is **permutation invariant**, meaning it does not inherently understand the order of tokens in a sequence. To address this, we need to encode the positions of words in a sequence so that the model can differentiate between tokens based on their order. The same embedding vector for a word should be treated differently depending on its position in the sequence.\n",
    "\n",
    "Thus, we will inject additional positional information into the token embeddings before feeding them into the LLM. We will use two broad approaches of position-aware embeddings:\n",
    "- relative position embeddings\n",
    "- absolute position embeddings\n",
    "\n",
    "#### Absolute Position Embeddings\n",
    "In absolute position embeddings, each position in the input sequence is assigned a unique embedding vector based on its absolute position (e.g., first word, second word, etc.).\n",
    "\n",
    "#### Relative Position Embeddings\n",
    "In relative position embeddings, the model learns to represent the positions of tokens relative to each other, rather than based on their absolute positions in the sequence. This allows the model to focus on the relationships between tokens regardless of their specific positions.\n",
    "\n",
    "The advantage of relative position embeddings is that the model can generalize better to sequences of varying lengths even if it has not seen such lengths during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19442fa0",
   "metadata": {},
   "source": [
    "OpenAI's GPT models use absolute position embeddings that are optimized during the training process rather than being fixed or predefined like the sinusoidal position embeddings used in the original Transformer model.\n",
    "\n",
    "Suppose we will encode the input tokens into a 256-dimensional embedding space and assume the token IDs were created by the BPE tokenizer from the previous section, which has a vocabulary size of 50257 (including the `<|endoftext|>` token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea24b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embed_layer = torch.nn.Embedding(\n",
    "    num_embeddings=vocab_size,\n",
    "    embedding_dim=output_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f41c632",
   "metadata": {},
   "source": [
    "If we sample data from the dataloader we created earlier, we will embed each token in each batch into a 256-dimensional vector using the token embedding layer we defined above.\n",
    "\n",
    "If we have a batch size of 8 with 4 tokens each (context size of 4), the resulting tensor of embedded tokens will have a shape of `(8, 4, 256)`, where:\n",
    "- `8` is the batch size,\n",
    "- `4` is the context size (number of tokens per sequence),\n",
    "- `256` is the embedding dimension for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d765470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      "tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      "torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "batch_size = 8\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,\n",
    "    batch_size=batch_size,\n",
    "    max_length=max_length,\n",
    "    stride=max_length,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(f\"Token IDs:\\n{inputs}\")\n",
    "print(f\"\\nInputs shape:\\n{inputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f62258b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Embeddings shape:\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embed_layer(inputs)\n",
    "print(f\"Token Embeddings shape:\\n{token_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5990ba",
   "metadata": {},
   "source": [
    "For the absolute position embeddings in a GPT model, we need to create another embedding layer that has the same embedding dimension as the `token_embed_layer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dc45f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position Embeddings shape:\n",
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embed_layer = torch.nn.Embedding(\n",
    "    num_embeddings=context_length,\n",
    "    embedding_dim=output_dim\n",
    ")\n",
    "\n",
    "# Create position embeddings for each position in the sequence\n",
    "pos_embeddings = pos_embed_layer(torch.arange(context_length))\n",
    "print(f\"Position Embeddings shape:\\n{pos_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4657d5d4",
   "metadata": {},
   "source": [
    "The input to the `pos_embed_layer` is a placeholder vector `torch.arange(context_length)` that represents the positions in the sequence from `0` to `context_length - 1`. This tensor is used to generate position embeddings for each position in the input sequence.\n",
    "\n",
    "In this case, we choose `context_length` to be equal to `max_length`, where the maximum input context size is equal to the maximum number of tokens the model can process in a single forward pass. In practice, input text can be longer than the supported context size, in which case we have to truncate the text.\n",
    "\n",
    "Next, we add the position embeddings to the token embeddings to create position-aware token embeddings that can be fed into the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "567cd0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embeddings shape:\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(f\"Input Embeddings shape:\\n{input_embeddings.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
